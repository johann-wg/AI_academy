{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.24.1.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from plotly.offline import init_notebook_mode,iplot\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.rcParams['font.family'] = 'Malgun Gothic'\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F_RunState</th>\n",
       "      <th>F_CycleTime</th>\n",
       "      <th>F_SpindleRPM1</th>\n",
       "      <th>F_SpindleTroq1</th>\n",
       "      <th>F_SpindleGearRatio1</th>\n",
       "      <th>F_ToolNum</th>\n",
       "      <th>G_ADC1</th>\n",
       "      <th>G_ADC2</th>\n",
       "      <th>G_ADC3</th>\n",
       "      <th>G_MV</th>\n",
       "      <th>G_MA</th>\n",
       "      <th>G_MActP</th>\n",
       "      <th>G_MFeq</th>\n",
       "      <th>G_MTemp</th>\n",
       "      <th>label</th>\n",
       "      <th>anomaly</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GDatetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-06-05 07:03:45</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.400000</td>\n",
       "      <td>43.700001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>222.020004</td>\n",
       "      <td>2.974</td>\n",
       "      <td>0.924</td>\n",
       "      <td>59.970001</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-05 07:03:46</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.400000</td>\n",
       "      <td>43.700001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>221.529999</td>\n",
       "      <td>2.976</td>\n",
       "      <td>0.924</td>\n",
       "      <td>59.970001</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-05 07:03:47</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.400000</td>\n",
       "      <td>43.700001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>221.529999</td>\n",
       "      <td>2.976</td>\n",
       "      <td>0.924</td>\n",
       "      <td>59.970001</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-05 07:03:48</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.400000</td>\n",
       "      <td>43.700001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>220.729996</td>\n",
       "      <td>2.976</td>\n",
       "      <td>0.920</td>\n",
       "      <td>59.959999</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-05 07:03:49</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.299999</td>\n",
       "      <td>43.700001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>221.630005</td>\n",
       "      <td>2.962</td>\n",
       "      <td>0.920</td>\n",
       "      <td>59.959999</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-01 16:27:16</th>\n",
       "      <td>2.0</td>\n",
       "      <td>10424.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>712.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>37.200001</td>\n",
       "      <td>38.900002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>220.570007</td>\n",
       "      <td>11.092</td>\n",
       "      <td>2.656</td>\n",
       "      <td>59.970001</td>\n",
       "      <td>45.0</td>\n",
       "      <td>363</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-01 16:27:17</th>\n",
       "      <td>2.0</td>\n",
       "      <td>10424.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>712.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>37.200001</td>\n",
       "      <td>38.900002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>220.669998</td>\n",
       "      <td>11.046</td>\n",
       "      <td>2.644</td>\n",
       "      <td>59.970001</td>\n",
       "      <td>45.0</td>\n",
       "      <td>363</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-01 16:27:18</th>\n",
       "      <td>2.0</td>\n",
       "      <td>10424.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>712.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>37.200001</td>\n",
       "      <td>38.900002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>220.669998</td>\n",
       "      <td>11.046</td>\n",
       "      <td>2.644</td>\n",
       "      <td>59.970001</td>\n",
       "      <td>45.0</td>\n",
       "      <td>363</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-01 16:27:19</th>\n",
       "      <td>2.0</td>\n",
       "      <td>10424.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>712.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>37.099998</td>\n",
       "      <td>38.900002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>220.960007</td>\n",
       "      <td>11.102</td>\n",
       "      <td>2.660</td>\n",
       "      <td>59.970001</td>\n",
       "      <td>45.0</td>\n",
       "      <td>363</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-01 16:27:20</th>\n",
       "      <td>2.0</td>\n",
       "      <td>10424.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>712.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>37.200001</td>\n",
       "      <td>38.900002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>220.779999</td>\n",
       "      <td>11.101</td>\n",
       "      <td>2.664</td>\n",
       "      <td>59.970001</td>\n",
       "      <td>45.0</td>\n",
       "      <td>363</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1621788 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     F_RunState  F_CycleTime  F_SpindleRPM1  F_SpindleTroq1  \\\n",
       "GDatetime                                                                     \n",
       "2023-06-05 07:03:45         2.0          0.0            0.0             0.0   \n",
       "2023-06-05 07:03:46         2.0          0.0            0.0             0.0   \n",
       "2023-06-05 07:03:47         2.0          0.0            0.0             0.0   \n",
       "2023-06-05 07:03:48         2.0          0.0            0.0             0.0   \n",
       "2023-06-05 07:03:49         2.0          0.0            0.0             0.0   \n",
       "...                         ...          ...            ...             ...   \n",
       "2023-07-01 16:27:16         2.0      10424.0            0.0             0.0   \n",
       "2023-07-01 16:27:17         2.0      10424.0            0.0             0.0   \n",
       "2023-07-01 16:27:18         2.0      10424.0            0.0             0.0   \n",
       "2023-07-01 16:27:19         2.0      10424.0            0.0             0.0   \n",
       "2023-07-01 16:27:20         2.0      10424.0            0.0             0.0   \n",
       "\n",
       "                     F_SpindleGearRatio1  F_ToolNum     G_ADC1     G_ADC2  \\\n",
       "GDatetime                                                                   \n",
       "2023-06-05 07:03:45                  0.0        0.0  24.400000  43.700001   \n",
       "2023-06-05 07:03:46                  0.0        0.0  24.400000  43.700001   \n",
       "2023-06-05 07:03:47                  0.0        0.0  24.400000  43.700001   \n",
       "2023-06-05 07:03:48                  0.0        0.0  24.400000  43.700001   \n",
       "2023-06-05 07:03:49                  0.0        0.0  24.299999  43.700001   \n",
       "...                                  ...        ...        ...        ...   \n",
       "2023-07-01 16:27:16                712.0       21.0  37.200001  38.900002   \n",
       "2023-07-01 16:27:17                712.0       21.0  37.200001  38.900002   \n",
       "2023-07-01 16:27:18                712.0       21.0  37.200001  38.900002   \n",
       "2023-07-01 16:27:19                712.0       21.0  37.099998  38.900002   \n",
       "2023-07-01 16:27:20                712.0       21.0  37.200001  38.900002   \n",
       "\n",
       "                     G_ADC3        G_MV    G_MA  G_MActP     G_MFeq  G_MTemp  \\\n",
       "GDatetime                                                                      \n",
       "2023-06-05 07:03:45     0.0  222.020004   2.974    0.924  59.970001     39.0   \n",
       "2023-06-05 07:03:46     0.0  221.529999   2.976    0.924  59.970001     39.0   \n",
       "2023-06-05 07:03:47     0.0  221.529999   2.976    0.924  59.970001     39.0   \n",
       "2023-06-05 07:03:48     0.0  220.729996   2.976    0.920  59.959999     39.0   \n",
       "2023-06-05 07:03:49     0.0  221.630005   2.962    0.920  59.959999     39.0   \n",
       "...                     ...         ...     ...      ...        ...      ...   \n",
       "2023-07-01 16:27:16     0.0  220.570007  11.092    2.656  59.970001     45.0   \n",
       "2023-07-01 16:27:17     0.0  220.669998  11.046    2.644  59.970001     45.0   \n",
       "2023-07-01 16:27:18     0.0  220.669998  11.046    2.644  59.970001     45.0   \n",
       "2023-07-01 16:27:19     0.0  220.960007  11.102    2.660  59.970001     45.0   \n",
       "2023-07-01 16:27:20     0.0  220.779999  11.101    2.664  59.970001     45.0   \n",
       "\n",
       "                     label  anomaly  \n",
       "GDatetime                            \n",
       "2023-06-05 07:03:45      0        0  \n",
       "2023-06-05 07:03:46      0        0  \n",
       "2023-06-05 07:03:47      0        0  \n",
       "2023-06-05 07:03:48      0        0  \n",
       "2023-06-05 07:03:49      0        0  \n",
       "...                    ...      ...  \n",
       "2023-07-01 16:27:16    363        0  \n",
       "2023-07-01 16:27:17    363        0  \n",
       "2023-07-01 16:27:18    363        0  \n",
       "2023-07-01 16:27:19    363        0  \n",
       "2023-07-01 16:27:20    363        0  \n",
       "\n",
       "[1621788 rows x 16 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('selected_data.csv')\n",
    "data['GDatetime'] = pd.to_datetime(data['GDatetime'])\n",
    "data.set_index('GDatetime', inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_cycle_periods(data, threshold_low=100, threshold_high=10000, sub_cycle_threshold=1000, sub_cycle_length_minutes=50):\n",
    "    cycle_starts = []\n",
    "    cycle_ends = []\n",
    "    in_cycle = False\n",
    "    in_high = False\n",
    "    in_sub_cycle = False\n",
    "    label = -1 # 주기 라벨 (0부터 시작)\n",
    "    \n",
    "    # label 열을 추가하고 -1로 초기화\n",
    "    data['label'] = 0\n",
    "    sub_cycle_start_time = None\n",
    "    sub_cycle_length = 0\n",
    "    \n",
    "    for i in range(len(data) - 1):\n",
    "        value = data['F_CycleTime'].iloc[i]\n",
    "        next_value = data['F_CycleTime'].iloc[i + 1]\n",
    "        \n",
    "        # 주기 시작 지점 찾기 (0 ~ 100 근처에서 올라가기 시작)\n",
    "        if not in_cycle and value <= threshold_low:\n",
    "            in_cycle = True\n",
    "            label += 1 # 새로운 주기 시작, 라벨 증가\n",
    "            cycle_starts.append(data.index[i]) # datetime 인덱스 사용\n",
    "        \n",
    "        # 주기 내부라면 현재 라벨로 표시\n",
    "        if in_cycle:\n",
    "            data['label'].iloc[i] = label\n",
    "\n",
    "        # 1만 근처의 값에 도달\n",
    "        if in_cycle and not in_high and value >= threshold_high:\n",
    "            in_high = True\n",
    "\n",
    "        # 1천 근처의 값에 도달\n",
    "        if in_cycle and not in_high and value >= sub_cycle_threshold:\n",
    "            in_sub_cycle = True\n",
    "            if sub_cycle_start_time is None:\n",
    "                sub_cycle_start_time = data.index[i]\n",
    "\n",
    "        # 하위 주기의 끝 지점 찾기\n",
    "        if in_sub_cycle and next_value <= threshold_low:\n",
    "            sub_cycle_length += (data.index[i] - sub_cycle_start_time).seconds / 60\n",
    "            sub_cycle_start_time = None\n",
    "            if sub_cycle_length >= sub_cycle_length_minutes:\n",
    "                in_high = True\n",
    "            in_sub_cycle = False\n",
    "            \n",
    "        # 주기 끝 지점 찾기\n",
    "        if in_high and next_value <= threshold_low:\n",
    "            in_high = False\n",
    "            in_cycle = False\n",
    "            cycle_ends.append(data.index[i]) # datetime 인덱스 사용\n",
    "            sub_cycle_length = 0\n",
    "            \n",
    "    # 마지막 주기의 끝 지점 처리\n",
    "    if in_cycle:\n",
    "        cycle_ends.append(data.index[-1])\n",
    "\n",
    "    return cycle_starts, cycle_ends\n",
    "\n",
    "\n",
    "def remove_constant_values(data, lower_bound=1000, upper_bound=12000, duration_minutes=20):\n",
    "    constant_value = None\n",
    "    constant_start_time = None\n",
    "    rows_to_drop = []\n",
    "\n",
    "    for i in range(len(data) - 1):\n",
    "        value = data['F_CycleTime'].iloc[i]\n",
    "\n",
    "        # 값이 범위 내에 있고 이전 값과 동일한 경우\n",
    "        if lower_bound <= value < upper_bound and value == constant_value:\n",
    "            if constant_start_time is None:\n",
    "                constant_start_time = data.index[i]\n",
    "            # 지속 시간이 20분 이상인 경우\n",
    "            if (data.index[i] - constant_start_time).seconds / 60 >= duration_minutes:\n",
    "                rows_to_drop.append(data.index[i])\n",
    "        else:\n",
    "            constant_value = value\n",
    "            constant_start_time = None\n",
    "\n",
    "    # 행 삭제\n",
    "    data.drop(rows_to_drop, inplace=True)\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "selected_data = data\n",
    "\n",
    "selected_data = remove_constant_values(selected_data)\n",
    "\n",
    "# 중복된 인덱스를 가진 행의 평균값으로 병합\n",
    "selected_data = selected_data.groupby(selected_data.index).mean()\n",
    "\n",
    "cycle_starts, cycle_ends = find_cycle_periods(selected_data)\n",
    "\n",
    "selected_data = selected_data.iloc[:-1]\n",
    "selected_data['anomaly'] = 0\n",
    "specific_labels = [43, 188, 243, 256, 258, 270, 291, 295, 325, 340, 349, 360]\n",
    "selected_data.loc[selected_data['label'].isin(specific_labels), 'anomaly'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature_columns에 넣고 싶은 특징 넣으면 됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((364, 7005, 11), (364,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 최대 샘플 길이를 정의 (선택적)\n",
    "max_length = max(len(selected_data.loc[start:end]) for start, end in zip(cycle_starts, cycle_ends))\n",
    "\n",
    "def create_3d_array(data, cycle_starts, cycle_ends, max_length=None, \n",
    "                    feature_columns=['F_RunState', 'F_ToolNum', 'F_CycleTime', 'F_SpindleRPM1', 'F_SpindleTroq1', 'G_ADC1', 'G_ADC2', 'G_ADC3', 'G_MV', 'G_MActP', 'G_MTemp', 'anomaly']):\n",
    "    # 주기별 샘플을 저장할 리스트\n",
    "    samples = []\n",
    "\n",
    "    # 각 주기를 샘플로 변환\n",
    "    for start, end in zip(cycle_starts, cycle_ends):\n",
    "        sample = data.loc[start:end][feature_columns].values\n",
    "        \n",
    "        # 샘플 길이 통일 (선택적)\n",
    "        if max_length:\n",
    "            if len(sample) > max_length:\n",
    "                sample = sample[:max_length]\n",
    "            elif len(sample) < max_length:\n",
    "                padding = np.zeros((max_length - len(sample), len(feature_columns)))\n",
    "                sample = np.vstack((sample, padding))\n",
    "        \n",
    "        samples.append(sample)\n",
    "\n",
    "    # 샘플들을 3차원 배열로 쌓기\n",
    "    samples_array = np.stack(samples)\n",
    "    \n",
    "    return samples_array\n",
    "\n",
    "# 3차원 배열 생성\n",
    "samples_array = create_3d_array(selected_data, cycle_starts, cycle_ends, max_length=max_length)\n",
    "\n",
    "# 결과의 형태 출력\n",
    "X = samples_array[:,:,:-1]\n",
    "y = samples_array[:,0,-1]\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 280은 마지막 1주 정도를 test set으로 사용하기 위함.\n",
    "# 스케일러, split 원하는 대로 알아서 해보길."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_val = X[:280]\n",
    "y_train_val = y[:280]\n",
    "X_test = X[280:]\n",
    "y_test = y[280:]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.3, stratify=y_train_val, random_state=42)\n",
    "\n",
    "std = StandardScaler()\n",
    "mm = MinMaxScaler()\n",
    "\n",
    "# 초기화\n",
    "X_train_sc = []\n",
    "X_val_sc = []\n",
    "X_test_sc = []\n",
    "\n",
    "# 학습 데이터 변환\n",
    "for sample in X_train:\n",
    "    mm_sample = mm.fit_transform(sample[:, :2])\n",
    "    std_sample = std.fit_transform(sample[:, 2:])\n",
    "    sc_sample = np.concatenate((mm_sample, std_sample), axis=1)\n",
    "    X_train_sc.append(sc_sample)\n",
    "\n",
    "# 검증 데이터 변환\n",
    "for sample in X_val:\n",
    "    mm_sample = mm.transform(sample[:, :2])\n",
    "    std_sample = std.transform(sample[:, 2:])\n",
    "    sc_sample = np.concatenate((mm_sample, std_sample), axis=1)\n",
    "    X_val_sc.append(sc_sample)\n",
    "\n",
    "# 테스트 데이터 변환\n",
    "for sample in X_test:\n",
    "    mm_sample = mm.transform(sample[:, :2])\n",
    "    std_sample = std.transform(sample[:, 2:])\n",
    "    sc_sample = np.concatenate((mm_sample, std_sample), axis=1)\n",
    "    X_test_sc.append(sc_sample)\n",
    "\n",
    "# NumPy 배열로 변환\n",
    "X_train_sc = np.array(X_train_sc)\n",
    "X_val_sc = np.array(X_val_sc)\n",
    "X_test_sc = np.array(X_test_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 7005, 11)]        0         \n",
      "                                                                 \n",
      " conv1d_12 (Conv1D)          (None, 7005, 64)          2176      \n",
      "                                                                 \n",
      " max_pooling1d_6 (MaxPoolin  (None, 3503, 64)          0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " conv1d_13 (Conv1D)          (None, 3503, 32)          6176      \n",
      "                                                                 \n",
      " max_pooling1d_7 (MaxPoolin  (None, 1752, 32)          0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " conv1d_transpose_3 (Conv1D  (None, 1752, 32)          3104      \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " up_sampling1d_6 (UpSamplin  (None, 3504, 32)          0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " conv1d_transpose_4 (Conv1D  (None, 3504, 64)          6208      \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " up_sampling1d_7 (UpSamplin  (None, 7008, 64)          0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " conv1d_transpose_5 (Conv1D  (None, 7008, 11)          2123      \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " cropping1d_1 (Cropping1D)   (None, 7005, 11)          0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 19787 (77.29 KB)\n",
      "Trainable params: 19787 (77.29 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, Conv1DTranspose, MaxPooling1D, UpSampling1D, Cropping1D\n",
    "\n",
    "# 입력 형태 설정\n",
    "input_shape = (7005, 11)  # 샘플 길이 7005, 특징 개수 11\n",
    "\n",
    "# 인코더 부분\n",
    "input_layer = Input(shape=input_shape)\n",
    "x = Conv1D(64, 3, activation='relu', padding='same')(input_layer)\n",
    "x = MaxPooling1D(2, padding='same')(x)\n",
    "x = Conv1D(32, 3, activation='relu', padding='same')(x)\n",
    "encoded = MaxPooling1D(2, padding='same')(x)\n",
    "\n",
    "# 디코더 부분\n",
    "x = Conv1DTranspose(32, 3, activation='relu', padding='same')(encoded)\n",
    "x = UpSampling1D(2)(x)\n",
    "x = Conv1DTranspose(64, 3, activation='relu', padding='same')(x)\n",
    "x = UpSampling1D(2)(x)\n",
    "x = Conv1DTranspose(11, 3, activation='sigmoid', padding='same')(x)\n",
    "\n",
    "# 출력 크기를 원래 입력 크기와 동일하게 조정 (Cropping)\n",
    "decoded = Cropping1D(cropping=(0, 3))(x)  # 출력 크기를 7005로 조정\n",
    "\n",
    "# 모델 구성\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "\n",
    "# 모델 컴파일\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# 모델 구조 출력\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "2/2 [==============================] - 95s 2s/step - loss: 1.0457 - val_loss: 1.0271\n",
      "Epoch 2/300\n",
      "2/2 [==============================] - 2s 613ms/step - loss: 1.0251 - val_loss: 1.0061\n",
      "Epoch 3/300\n",
      "2/2 [==============================] - 2s 554ms/step - loss: 1.0037 - val_loss: 0.9790\n",
      "Epoch 4/300\n",
      "2/2 [==============================] - 2s 569ms/step - loss: 0.9758 - val_loss: 0.9426\n",
      "Epoch 5/300\n",
      "2/2 [==============================] - 2s 559ms/step - loss: 0.9383 - val_loss: 0.8957\n",
      "Epoch 6/300\n",
      "2/2 [==============================] - 2s 576ms/step - loss: 0.8905 - val_loss: 0.8399\n",
      "Epoch 7/300\n",
      "2/2 [==============================] - 2s 590ms/step - loss: 0.8339 - val_loss: 0.7795\n",
      "Epoch 8/300\n",
      "2/2 [==============================] - 2s 573ms/step - loss: 0.7727 - val_loss: 0.7195\n",
      "Epoch 9/300\n",
      "2/2 [==============================] - 2s 525ms/step - loss: 0.7126 - val_loss: 0.6676\n",
      "Epoch 10/300\n",
      "2/2 [==============================] - 2s 518ms/step - loss: 0.6611 - val_loss: 0.6285\n",
      "Epoch 11/300\n",
      "2/2 [==============================] - 2s 612ms/step - loss: 0.6228 - val_loss: 0.6015\n",
      "Epoch 12/300\n",
      "2/2 [==============================] - 2s 603ms/step - loss: 0.5960 - val_loss: 0.5826\n",
      "Epoch 13/300\n",
      "2/2 [==============================] - 2s 578ms/step - loss: 0.5776 - val_loss: 0.5690\n",
      "Epoch 14/300\n",
      "2/2 [==============================] - 2s 555ms/step - loss: 0.5644 - val_loss: 0.5584\n",
      "Epoch 15/300\n",
      "2/2 [==============================] - 2s 625ms/step - loss: 0.5540 - val_loss: 0.5505\n",
      "Epoch 16/300\n",
      "2/2 [==============================] - 2s 588ms/step - loss: 0.5464 - val_loss: 0.5452\n",
      "Epoch 17/300\n",
      "2/2 [==============================] - 2s 637ms/step - loss: 0.5413 - val_loss: 0.5412\n",
      "Epoch 18/300\n",
      "2/2 [==============================] - 2s 610ms/step - loss: 0.5372 - val_loss: 0.5369\n",
      "Epoch 19/300\n",
      "2/2 [==============================] - 2s 623ms/step - loss: 0.5329 - val_loss: 0.5332\n",
      "Epoch 20/300\n",
      "2/2 [==============================] - 2s 627ms/step - loss: 0.5295 - val_loss: 0.5301\n",
      "Epoch 21/300\n",
      "2/2 [==============================] - 2s 623ms/step - loss: 0.5264 - val_loss: 0.5281\n",
      "Epoch 22/300\n",
      "2/2 [==============================] - 2s 668ms/step - loss: 0.5246 - val_loss: 0.5265\n",
      "Epoch 23/300\n",
      "2/2 [==============================] - 3s 642ms/step - loss: 0.5231 - val_loss: 0.5252\n",
      "Epoch 24/300\n",
      "2/2 [==============================] - 2s 648ms/step - loss: 0.5219 - val_loss: 0.5242\n",
      "Epoch 25/300\n",
      "2/2 [==============================] - 2s 661ms/step - loss: 0.5209 - val_loss: 0.5231\n",
      "Epoch 26/300\n",
      "2/2 [==============================] - 3s 686ms/step - loss: 0.5199 - val_loss: 0.5225\n",
      "Epoch 27/300\n",
      "2/2 [==============================] - 3s 681ms/step - loss: 0.5193 - val_loss: 0.5220\n",
      "Epoch 28/300\n",
      "2/2 [==============================] - 2s 605ms/step - loss: 0.5188 - val_loss: 0.5211\n",
      "Epoch 29/300\n",
      "2/2 [==============================] - 2s 601ms/step - loss: 0.5180 - val_loss: 0.5205\n",
      "Epoch 30/300\n",
      "2/2 [==============================] - 2s 644ms/step - loss: 0.5174 - val_loss: 0.5203\n",
      "Epoch 31/300\n",
      "2/2 [==============================] - 2s 664ms/step - loss: 0.5171 - val_loss: 0.5195\n",
      "Epoch 32/300\n",
      "2/2 [==============================] - 2s 611ms/step - loss: 0.5164 - val_loss: 0.5190\n",
      "Epoch 33/300\n",
      "2/2 [==============================] - 2s 643ms/step - loss: 0.5160 - val_loss: 0.5186\n",
      "Epoch 34/300\n",
      "2/2 [==============================] - 2s 611ms/step - loss: 0.5155 - val_loss: 0.5182\n",
      "Epoch 35/300\n",
      "2/2 [==============================] - 2s 605ms/step - loss: 0.5152 - val_loss: 0.5179\n",
      "Epoch 36/300\n",
      "2/2 [==============================] - 2s 670ms/step - loss: 0.5148 - val_loss: 0.5175\n",
      "Epoch 37/300\n",
      "2/2 [==============================] - 2s 648ms/step - loss: 0.5144 - val_loss: 0.5172\n",
      "Epoch 38/300\n",
      "2/2 [==============================] - 2s 587ms/step - loss: 0.5142 - val_loss: 0.5170\n",
      "Epoch 39/300\n",
      "2/2 [==============================] - 2s 607ms/step - loss: 0.5140 - val_loss: 0.5166\n",
      "Epoch 40/300\n",
      "2/2 [==============================] - 2s 610ms/step - loss: 0.5137 - val_loss: 0.5163\n",
      "Epoch 41/300\n",
      "2/2 [==============================] - 2s 602ms/step - loss: 0.5134 - val_loss: 0.5161\n",
      "Epoch 42/300\n",
      "2/2 [==============================] - 2s 645ms/step - loss: 0.5132 - val_loss: 0.5158\n",
      "Epoch 43/300\n",
      "2/2 [==============================] - 2s 635ms/step - loss: 0.5129 - val_loss: 0.5156\n",
      "Epoch 44/300\n",
      "2/2 [==============================] - 2s 630ms/step - loss: 0.5127 - val_loss: 0.5154\n",
      "Epoch 45/300\n",
      "2/2 [==============================] - 2s 641ms/step - loss: 0.5125 - val_loss: 0.5151\n",
      "Epoch 46/300\n",
      "2/2 [==============================] - 2s 642ms/step - loss: 0.5123 - val_loss: 0.5150\n",
      "Epoch 47/300\n",
      "2/2 [==============================] - 2s 618ms/step - loss: 0.5122 - val_loss: 0.5151\n",
      "Epoch 48/300\n",
      "2/2 [==============================] - 2s 647ms/step - loss: 0.5121 - val_loss: 0.5146\n",
      "Epoch 49/300\n",
      "2/2 [==============================] - 2s 592ms/step - loss: 0.5118 - val_loss: 0.5144\n",
      "Epoch 50/300\n",
      "2/2 [==============================] - 2s 611ms/step - loss: 0.5115 - val_loss: 0.5142\n",
      "Epoch 51/300\n",
      "2/2 [==============================] - 2s 631ms/step - loss: 0.5114 - val_loss: 0.5138\n",
      "Epoch 52/300\n",
      "2/2 [==============================] - 2s 629ms/step - loss: 0.5110 - val_loss: 0.5137\n",
      "Epoch 53/300\n",
      "2/2 [==============================] - 2s 589ms/step - loss: 0.5109 - val_loss: 0.5137\n",
      "Epoch 54/300\n",
      "2/2 [==============================] - 2s 601ms/step - loss: 0.5108 - val_loss: 0.5134\n",
      "Epoch 55/300\n",
      "2/2 [==============================] - 2s 655ms/step - loss: 0.5106 - val_loss: 0.5133\n",
      "Epoch 56/300\n",
      "2/2 [==============================] - 2s 638ms/step - loss: 0.5104 - val_loss: 0.5129\n",
      "Epoch 57/300\n",
      "2/2 [==============================] - 2s 626ms/step - loss: 0.5102 - val_loss: 0.5127\n",
      "Epoch 58/300\n",
      "2/2 [==============================] - 2s 632ms/step - loss: 0.5099 - val_loss: 0.5127\n",
      "Epoch 59/300\n",
      "2/2 [==============================] - 2s 618ms/step - loss: 0.5100 - val_loss: 0.5123\n",
      "Epoch 60/300\n",
      "2/2 [==============================] - 2s 617ms/step - loss: 0.5096 - val_loss: 0.5120\n",
      "Epoch 61/300\n",
      "2/2 [==============================] - 2s 602ms/step - loss: 0.5094 - val_loss: 0.5121\n",
      "Epoch 62/300\n",
      "2/2 [==============================] - 2s 634ms/step - loss: 0.5093 - val_loss: 0.5118\n",
      "Epoch 63/300\n",
      "2/2 [==============================] - 2s 629ms/step - loss: 0.5091 - val_loss: 0.5116\n",
      "Epoch 64/300\n",
      "2/2 [==============================] - 2s 602ms/step - loss: 0.5089 - val_loss: 0.5114\n",
      "Epoch 65/300\n",
      "2/2 [==============================] - 2s 678ms/step - loss: 0.5087 - val_loss: 0.5113\n",
      "Epoch 66/300\n",
      "2/2 [==============================] - 3s 651ms/step - loss: 0.5086 - val_loss: 0.5111\n",
      "Epoch 67/300\n",
      "2/2 [==============================] - 2s 625ms/step - loss: 0.5084 - val_loss: 0.5108\n",
      "Epoch 68/300\n",
      "2/2 [==============================] - 2s 602ms/step - loss: 0.5082 - val_loss: 0.5107\n",
      "Epoch 69/300\n",
      "2/2 [==============================] - 2s 562ms/step - loss: 0.5080 - val_loss: 0.5105\n",
      "Epoch 70/300\n",
      "2/2 [==============================] - 2s 606ms/step - loss: 0.5079 - val_loss: 0.5104\n",
      "Epoch 71/300\n",
      "2/2 [==============================] - 2s 535ms/step - loss: 0.5077 - val_loss: 0.5102\n",
      "Epoch 72/300\n",
      "2/2 [==============================] - 2s 496ms/step - loss: 0.5076 - val_loss: 0.5101\n",
      "Epoch 73/300\n",
      "2/2 [==============================] - 2s 539ms/step - loss: 0.5075 - val_loss: 0.5099\n",
      "Epoch 74/300\n",
      "2/2 [==============================] - 2s 542ms/step - loss: 0.5073 - val_loss: 0.5098\n",
      "Epoch 75/300\n",
      "2/2 [==============================] - 2s 561ms/step - loss: 0.5072 - val_loss: 0.5096\n",
      "Epoch 76/300\n",
      "2/2 [==============================] - 2s 606ms/step - loss: 0.5070 - val_loss: 0.5097\n",
      "Epoch 77/300\n",
      "2/2 [==============================] - 2s 568ms/step - loss: 0.5070 - val_loss: 0.5093\n",
      "Epoch 78/300\n",
      "2/2 [==============================] - 2s 627ms/step - loss: 0.5067 - val_loss: 0.5092\n",
      "Epoch 79/300\n",
      "2/2 [==============================] - 2s 611ms/step - loss: 0.5066 - val_loss: 0.5091\n",
      "Epoch 80/300\n",
      "2/2 [==============================] - 2s 646ms/step - loss: 0.5065 - val_loss: 0.5090\n",
      "Epoch 81/300\n",
      "2/2 [==============================] - 2s 629ms/step - loss: 0.5064 - val_loss: 0.5089\n",
      "Epoch 82/300\n",
      "2/2 [==============================] - 2s 595ms/step - loss: 0.5063 - val_loss: 0.5089\n",
      "Epoch 83/300\n",
      "2/2 [==============================] - 2s 607ms/step - loss: 0.5064 - val_loss: 0.5087\n",
      "Epoch 84/300\n",
      "2/2 [==============================] - 2s 645ms/step - loss: 0.5061 - val_loss: 0.5088\n",
      "Epoch 85/300\n",
      "2/2 [==============================] - 2s 587ms/step - loss: 0.5062 - val_loss: 0.5087\n",
      "Epoch 86/300\n",
      "2/2 [==============================] - 2s 592ms/step - loss: 0.5062 - val_loss: 0.5084\n",
      "Epoch 87/300\n",
      "2/2 [==============================] - 2s 604ms/step - loss: 0.5059 - val_loss: 0.5086\n",
      "Epoch 88/300\n",
      "2/2 [==============================] - 2s 612ms/step - loss: 0.5060 - val_loss: 0.5083\n",
      "Epoch 89/300\n",
      "2/2 [==============================] - 2s 600ms/step - loss: 0.5058 - val_loss: 0.5081\n",
      "Epoch 90/300\n",
      "2/2 [==============================] - 2s 599ms/step - loss: 0.5056 - val_loss: 0.5081\n",
      "Epoch 91/300\n",
      "2/2 [==============================] - 2s 628ms/step - loss: 0.5055 - val_loss: 0.5080\n",
      "Epoch 92/300\n",
      "2/2 [==============================] - 3s 654ms/step - loss: 0.5054 - val_loss: 0.5079\n",
      "Epoch 93/300\n",
      "2/2 [==============================] - 2s 644ms/step - loss: 0.5053 - val_loss: 0.5079\n",
      "Epoch 94/300\n",
      "2/2 [==============================] - 2s 598ms/step - loss: 0.5053 - val_loss: 0.5077\n",
      "Epoch 95/300\n",
      "2/2 [==============================] - 2s 578ms/step - loss: 0.5052 - val_loss: 0.5077\n",
      "Epoch 96/300\n",
      "2/2 [==============================] - 2s 595ms/step - loss: 0.5052 - val_loss: 0.5076\n",
      "Epoch 97/300\n",
      "2/2 [==============================] - 2s 588ms/step - loss: 0.5051 - val_loss: 0.5076\n",
      "Epoch 98/300\n",
      "2/2 [==============================] - 2s 690ms/step - loss: 0.5051 - val_loss: 0.5075\n",
      "Epoch 99/300\n",
      "2/2 [==============================] - 2s 588ms/step - loss: 0.5049 - val_loss: 0.5074\n",
      "Epoch 100/300\n",
      "2/2 [==============================] - 2s 608ms/step - loss: 0.5049 - val_loss: 0.5073\n",
      "Epoch 101/300\n",
      "2/2 [==============================] - 2s 606ms/step - loss: 0.5048 - val_loss: 0.5074\n",
      "Epoch 102/300\n",
      "2/2 [==============================] - 2s 601ms/step - loss: 0.5048 - val_loss: 0.5071\n",
      "Epoch 103/300\n",
      "2/2 [==============================] - 2s 598ms/step - loss: 0.5046 - val_loss: 0.5070\n",
      "Epoch 104/300\n",
      "2/2 [==============================] - 2s 590ms/step - loss: 0.5045 - val_loss: 0.5070\n",
      "Epoch 105/300\n",
      "2/2 [==============================] - 2s 601ms/step - loss: 0.5045 - val_loss: 0.5070\n",
      "Epoch 106/300\n",
      "2/2 [==============================] - 2s 597ms/step - loss: 0.5044 - val_loss: 0.5069\n",
      "Epoch 107/300\n",
      "2/2 [==============================] - 2s 587ms/step - loss: 0.5044 - val_loss: 0.5067\n",
      "Epoch 108/300\n",
      "2/2 [==============================] - 2s 582ms/step - loss: 0.5043 - val_loss: 0.5067\n",
      "Epoch 109/300\n",
      "2/2 [==============================] - 2s 590ms/step - loss: 0.5042 - val_loss: 0.5066\n",
      "Epoch 110/300\n",
      "2/2 [==============================] - 2s 577ms/step - loss: 0.5041 - val_loss: 0.5065\n",
      "Epoch 111/300\n",
      "2/2 [==============================] - 2s 598ms/step - loss: 0.5040 - val_loss: 0.5065\n",
      "Epoch 112/300\n",
      "2/2 [==============================] - 2s 610ms/step - loss: 0.5040 - val_loss: 0.5065\n",
      "Epoch 113/300\n",
      "2/2 [==============================] - 2s 596ms/step - loss: 0.5040 - val_loss: 0.5069\n",
      "Epoch 114/300\n",
      "2/2 [==============================] - 2s 628ms/step - loss: 0.5042 - val_loss: 0.5063\n",
      "Epoch 115/300\n",
      "2/2 [==============================] - 2s 611ms/step - loss: 0.5039 - val_loss: 0.5062\n",
      "Epoch 116/300\n",
      "2/2 [==============================] - 2s 652ms/step - loss: 0.5038 - val_loss: 0.5061\n",
      "Epoch 117/300\n",
      "2/2 [==============================] - 2s 618ms/step - loss: 0.5037 - val_loss: 0.5061\n",
      "Epoch 118/300\n",
      "2/2 [==============================] - 2s 644ms/step - loss: 0.5036 - val_loss: 0.5064\n",
      "Epoch 119/300\n",
      "2/2 [==============================] - 3s 627ms/step - loss: 0.5038 - val_loss: 0.5060\n",
      "Epoch 120/300\n",
      "2/2 [==============================] - 2s 616ms/step - loss: 0.5036 - val_loss: 0.5059\n",
      "Epoch 121/300\n",
      "2/2 [==============================] - 2s 618ms/step - loss: 0.5035 - val_loss: 0.5063\n",
      "Epoch 122/300\n",
      "2/2 [==============================] - 2s 620ms/step - loss: 0.5036 - val_loss: 0.5061\n",
      "Epoch 123/300\n",
      "2/2 [==============================] - 2s 638ms/step - loss: 0.5037 - val_loss: 0.5060\n",
      "Epoch 124/300\n",
      "2/2 [==============================] - 2s 582ms/step - loss: 0.5035 - val_loss: 0.5061\n",
      "Epoch 125/300\n",
      "2/2 [==============================] - 2s 604ms/step - loss: 0.5035 - val_loss: 0.5058\n",
      "Epoch 126/300\n",
      "2/2 [==============================] - 2s 641ms/step - loss: 0.5034 - val_loss: 0.5056\n",
      "Epoch 127/300\n",
      "2/2 [==============================] - 2s 608ms/step - loss: 0.5031 - val_loss: 0.5057\n",
      "Epoch 128/300\n",
      "2/2 [==============================] - 2s 596ms/step - loss: 0.5031 - val_loss: 0.5057\n",
      "Epoch 129/300\n",
      "2/2 [==============================] - 2s 611ms/step - loss: 0.5032 - val_loss: 0.5056\n",
      "Epoch 130/300\n",
      "2/2 [==============================] - 2s 580ms/step - loss: 0.5031 - val_loss: 0.5054\n",
      "Epoch 131/300\n",
      "2/2 [==============================] - 2s 604ms/step - loss: 0.5029 - val_loss: 0.5053\n",
      "Epoch 132/300\n",
      "2/2 [==============================] - 2s 600ms/step - loss: 0.5029 - val_loss: 0.5053\n",
      "Epoch 133/300\n",
      "2/2 [==============================] - 2s 603ms/step - loss: 0.5028 - val_loss: 0.5054\n",
      "Epoch 134/300\n",
      "2/2 [==============================] - 2s 592ms/step - loss: 0.5030 - val_loss: 0.5051\n",
      "Epoch 135/300\n",
      "2/2 [==============================] - 2s 668ms/step - loss: 0.5027 - val_loss: 0.5051\n",
      "Epoch 136/300\n",
      "2/2 [==============================] - 2s 580ms/step - loss: 0.5026 - val_loss: 0.5051\n",
      "Epoch 137/300\n",
      "2/2 [==============================] - 2s 590ms/step - loss: 0.5027 - val_loss: 0.5051\n",
      "Epoch 138/300\n",
      "2/2 [==============================] - 2s 596ms/step - loss: 0.5026 - val_loss: 0.5050\n",
      "Epoch 139/300\n",
      "2/2 [==============================] - 2s 578ms/step - loss: 0.5025 - val_loss: 0.5049\n",
      "Epoch 140/300\n",
      "2/2 [==============================] - 2s 598ms/step - loss: 0.5024 - val_loss: 0.5049\n",
      "Epoch 141/300\n",
      "2/2 [==============================] - 2s 639ms/step - loss: 0.5024 - val_loss: 0.5048\n",
      "Epoch 142/300\n",
      "2/2 [==============================] - 2s 587ms/step - loss: 0.5023 - val_loss: 0.5047\n",
      "Epoch 143/300\n",
      "2/2 [==============================] - 2s 627ms/step - loss: 0.5023 - val_loss: 0.5047\n",
      "Epoch 144/300\n",
      "2/2 [==============================] - 2s 606ms/step - loss: 0.5022 - val_loss: 0.5047\n",
      "Epoch 145/300\n",
      "2/2 [==============================] - 2s 650ms/step - loss: 0.5023 - val_loss: 0.5046\n",
      "Epoch 146/300\n",
      "2/2 [==============================] - 2s 617ms/step - loss: 0.5021 - val_loss: 0.5046\n",
      "Epoch 147/300\n",
      "2/2 [==============================] - 2s 600ms/step - loss: 0.5021 - val_loss: 0.5046\n",
      "Epoch 148/300\n",
      "2/2 [==============================] - 2s 605ms/step - loss: 0.5021 - val_loss: 0.5047\n",
      "Epoch 149/300\n",
      "2/2 [==============================] - 2s 603ms/step - loss: 0.5022 - val_loss: 0.5045\n",
      "Epoch 150/300\n",
      "2/2 [==============================] - 2s 546ms/step - loss: 0.5020 - val_loss: 0.5046\n",
      "Epoch 151/300\n",
      "2/2 [==============================] - 2s 561ms/step - loss: 0.5022 - val_loss: 0.5044\n",
      "Epoch 152/300\n",
      "2/2 [==============================] - 2s 596ms/step - loss: 0.5019 - val_loss: 0.5045\n",
      "Epoch 153/300\n",
      "2/2 [==============================] - 2s 576ms/step - loss: 0.5020 - val_loss: 0.5043\n",
      "Epoch 154/300\n",
      "2/2 [==============================] - 2s 586ms/step - loss: 0.5018 - val_loss: 0.5043\n",
      "Epoch 155/300\n",
      "2/2 [==============================] - 2s 563ms/step - loss: 0.5018 - val_loss: 0.5042\n",
      "Epoch 156/300\n",
      "2/2 [==============================] - 2s 543ms/step - loss: 0.5018 - val_loss: 0.5042\n",
      "Epoch 157/300\n",
      "2/2 [==============================] - 2s 550ms/step - loss: 0.5018 - val_loss: 0.5042\n",
      "Epoch 158/300\n",
      "2/2 [==============================] - 2s 565ms/step - loss: 0.5017 - val_loss: 0.5041\n",
      "Epoch 159/300\n",
      "2/2 [==============================] - 2s 570ms/step - loss: 0.5017 - val_loss: 0.5041\n",
      "Epoch 160/300\n",
      "2/2 [==============================] - 2s 563ms/step - loss: 0.5017 - val_loss: 0.5043\n",
      "Epoch 161/300\n",
      "2/2 [==============================] - 2s 564ms/step - loss: 0.5018 - val_loss: 0.5041\n",
      "Epoch 162/300\n",
      "2/2 [==============================] - 2s 563ms/step - loss: 0.5017 - val_loss: 0.5040\n",
      "Epoch 163/300\n",
      "2/2 [==============================] - 2s 578ms/step - loss: 0.5016 - val_loss: 0.5041\n",
      "Epoch 164/300\n",
      "2/2 [==============================] - 2s 543ms/step - loss: 0.5016 - val_loss: 0.5039\n",
      "Epoch 165/300\n",
      "2/2 [==============================] - 2s 583ms/step - loss: 0.5015 - val_loss: 0.5039\n",
      "Epoch 166/300\n",
      "2/2 [==============================] - 2s 648ms/step - loss: 0.5014 - val_loss: 0.5038\n",
      "Epoch 167/300\n",
      "2/2 [==============================] - 2s 603ms/step - loss: 0.5014 - val_loss: 0.5038\n",
      "Epoch 168/300\n",
      "2/2 [==============================] - 2s 605ms/step - loss: 0.5014 - val_loss: 0.5038\n",
      "Epoch 169/300\n",
      "2/2 [==============================] - 2s 576ms/step - loss: 0.5014 - val_loss: 0.5039\n",
      "Epoch 170/300\n",
      "2/2 [==============================] - 2s 545ms/step - loss: 0.5014 - val_loss: 0.5038\n",
      "Epoch 171/300\n",
      "2/2 [==============================] - 2s 561ms/step - loss: 0.5014 - val_loss: 0.5036\n",
      "Epoch 172/300\n",
      "2/2 [==============================] - 2s 556ms/step - loss: 0.5012 - val_loss: 0.5039\n",
      "Epoch 173/300\n",
      "2/2 [==============================] - 2s 571ms/step - loss: 0.5014 - val_loss: 0.5037\n",
      "Epoch 174/300\n",
      "2/2 [==============================] - 2s 584ms/step - loss: 0.5013 - val_loss: 0.5036\n",
      "Epoch 175/300\n",
      "2/2 [==============================] - 2s 589ms/step - loss: 0.5012 - val_loss: 0.5039\n",
      "Epoch 176/300\n",
      "2/2 [==============================] - 2s 567ms/step - loss: 0.5013 - val_loss: 0.5039\n",
      "Epoch 177/300\n",
      "2/2 [==============================] - 2s 548ms/step - loss: 0.5015 - val_loss: 0.5037\n",
      "Epoch 178/300\n",
      "2/2 [==============================] - 2s 564ms/step - loss: 0.5012 - val_loss: 0.5035\n",
      "Epoch 179/300\n",
      "2/2 [==============================] - 2s 575ms/step - loss: 0.5011 - val_loss: 0.5035\n",
      "Epoch 180/300\n",
      "2/2 [==============================] - 2s 553ms/step - loss: 0.5011 - val_loss: 0.5036\n",
      "Epoch 181/300\n",
      "2/2 [==============================] - 2s 575ms/step - loss: 0.5011 - val_loss: 0.5036\n",
      "Epoch 182/300\n",
      "2/2 [==============================] - 2s 584ms/step - loss: 0.5012 - val_loss: 0.5035\n",
      "Epoch 183/300\n",
      "2/2 [==============================] - 2s 564ms/step - loss: 0.5011 - val_loss: 0.5035\n",
      "Epoch 184/300\n",
      "2/2 [==============================] - 2s 560ms/step - loss: 0.5010 - val_loss: 0.5034\n",
      "Epoch 185/300\n",
      "2/2 [==============================] - 2s 569ms/step - loss: 0.5010 - val_loss: 0.5037\n",
      "Epoch 186/300\n",
      "2/2 [==============================] - 2s 567ms/step - loss: 0.5012 - val_loss: 0.5033\n",
      "Epoch 187/300\n",
      "2/2 [==============================] - 2s 577ms/step - loss: 0.5009 - val_loss: 0.5032\n",
      "Epoch 188/300\n",
      "2/2 [==============================] - 2s 556ms/step - loss: 0.5008 - val_loss: 0.5033\n",
      "Epoch 189/300\n",
      "2/2 [==============================] - 2s 559ms/step - loss: 0.5008 - val_loss: 0.5034\n",
      "Epoch 190/300\n",
      "2/2 [==============================] - 2s 582ms/step - loss: 0.5010 - val_loss: 0.5034\n",
      "Epoch 191/300\n",
      "2/2 [==============================] - 2s 593ms/step - loss: 0.5009 - val_loss: 0.5032\n",
      "Epoch 192/300\n",
      "2/2 [==============================] - 2s 526ms/step - loss: 0.5008 - val_loss: 0.5031\n",
      "Epoch 193/300\n",
      "2/2 [==============================] - 2s 535ms/step - loss: 0.5007 - val_loss: 0.5032\n",
      "Epoch 194/300\n",
      "2/2 [==============================] - 2s 525ms/step - loss: 0.5007 - val_loss: 0.5031\n",
      "Epoch 195/300\n",
      "2/2 [==============================] - 2s 544ms/step - loss: 0.5006 - val_loss: 0.5030\n",
      "Epoch 196/300\n",
      "2/2 [==============================] - 2s 566ms/step - loss: 0.5006 - val_loss: 0.5031\n",
      "Epoch 197/300\n",
      "2/2 [==============================] - 2s 535ms/step - loss: 0.5007 - val_loss: 0.5031\n",
      "Epoch 198/300\n",
      "2/2 [==============================] - 2s 590ms/step - loss: 0.5006 - val_loss: 0.5029\n",
      "Epoch 199/300\n",
      "2/2 [==============================] - 2s 516ms/step - loss: 0.5005 - val_loss: 0.5030\n",
      "Epoch 200/300\n",
      "2/2 [==============================] - 2s 562ms/step - loss: 0.5006 - val_loss: 0.5030\n",
      "Epoch 201/300\n",
      "2/2 [==============================] - 2s 537ms/step - loss: 0.5005 - val_loss: 0.5031\n",
      "Epoch 202/300\n",
      "2/2 [==============================] - 2s 622ms/step - loss: 0.5007 - val_loss: 0.5029\n",
      "Epoch 203/300\n",
      "2/2 [==============================] - 2s 643ms/step - loss: 0.5005 - val_loss: 0.5029\n",
      "Epoch 204/300\n",
      "2/2 [==============================] - 2s 574ms/step - loss: 0.5005 - val_loss: 0.5028\n",
      "Epoch 205/300\n",
      "2/2 [==============================] - 2s 542ms/step - loss: 0.5004 - val_loss: 0.5028\n",
      "Epoch 206/300\n",
      "2/2 [==============================] - 2s 549ms/step - loss: 0.5003 - val_loss: 0.5028\n",
      "Epoch 207/300\n",
      "2/2 [==============================] - 2s 564ms/step - loss: 0.5003 - val_loss: 0.5028\n",
      "Epoch 208/300\n",
      "2/2 [==============================] - 2s 547ms/step - loss: 0.5004 - val_loss: 0.5027\n",
      "Epoch 209/300\n",
      "2/2 [==============================] - 2s 551ms/step - loss: 0.5003 - val_loss: 0.5028\n",
      "Epoch 210/300\n",
      "2/2 [==============================] - 2s 561ms/step - loss: 0.5003 - val_loss: 0.5028\n",
      "Epoch 211/300\n",
      "2/2 [==============================] - 2s 543ms/step - loss: 0.5004 - val_loss: 0.5027\n",
      "Epoch 212/300\n",
      "2/2 [==============================] - 2s 568ms/step - loss: 0.5003 - val_loss: 0.5027\n",
      "Epoch 213/300\n",
      "2/2 [==============================] - 2s 574ms/step - loss: 0.5002 - val_loss: 0.5029\n",
      "Epoch 214/300\n",
      "2/2 [==============================] - 2s 570ms/step - loss: 0.5005 - val_loss: 0.5026\n",
      "Epoch 215/300\n",
      "2/2 [==============================] - 2s 542ms/step - loss: 0.5002 - val_loss: 0.5026\n",
      "Epoch 216/300\n",
      "2/2 [==============================] - 2s 551ms/step - loss: 0.5002 - val_loss: 0.5026\n",
      "Epoch 217/300\n",
      "2/2 [==============================] - 2s 541ms/step - loss: 0.5002 - val_loss: 0.5026\n",
      "Epoch 218/300\n",
      "2/2 [==============================] - 2s 579ms/step - loss: 0.5001 - val_loss: 0.5025\n",
      "Epoch 219/300\n",
      "2/2 [==============================] - 2s 576ms/step - loss: 0.5001 - val_loss: 0.5026\n",
      "Epoch 220/300\n",
      "2/2 [==============================] - 2s 560ms/step - loss: 0.5001 - val_loss: 0.5026\n",
      "Epoch 221/300\n",
      "2/2 [==============================] - 2s 616ms/step - loss: 0.5001 - val_loss: 0.5024\n",
      "Epoch 222/300\n",
      "2/2 [==============================] - 2s 542ms/step - loss: 0.5000 - val_loss: 0.5026\n",
      "Epoch 223/300\n",
      "2/2 [==============================] - 2s 542ms/step - loss: 0.5002 - val_loss: 0.5025\n",
      "Epoch 224/300\n",
      "2/2 [==============================] - 2s 534ms/step - loss: 0.5000 - val_loss: 0.5024\n",
      "Epoch 225/300\n",
      "2/2 [==============================] - 2s 599ms/step - loss: 0.5000 - val_loss: 0.5024\n",
      "Epoch 226/300\n",
      "2/2 [==============================] - 2s 552ms/step - loss: 0.5000 - val_loss: 0.5024\n",
      "Epoch 227/300\n",
      "2/2 [==============================] - 2s 533ms/step - loss: 0.5000 - val_loss: 0.5024\n",
      "Epoch 228/300\n",
      "2/2 [==============================] - 2s 563ms/step - loss: 0.5000 - val_loss: 0.5024\n",
      "Epoch 229/300\n",
      "2/2 [==============================] - 2s 610ms/step - loss: 0.4999 - val_loss: 0.5023\n",
      "Epoch 230/300\n",
      "2/2 [==============================] - 2s 593ms/step - loss: 0.4999 - val_loss: 0.5023\n",
      "Epoch 231/300\n",
      "2/2 [==============================] - 2s 592ms/step - loss: 0.5000 - val_loss: 0.5022\n",
      "Epoch 232/300\n",
      "2/2 [==============================] - 2s 534ms/step - loss: 0.4999 - val_loss: 0.5023\n",
      "Epoch 233/300\n",
      "2/2 [==============================] - 2s 620ms/step - loss: 0.4999 - val_loss: 0.5023\n",
      "Epoch 234/300\n",
      "2/2 [==============================] - 2s 565ms/step - loss: 0.4999 - val_loss: 0.5022\n",
      "Epoch 235/300\n",
      "2/2 [==============================] - 2s 555ms/step - loss: 0.4998 - val_loss: 0.5022\n",
      "Epoch 236/300\n",
      "2/2 [==============================] - 2s 530ms/step - loss: 0.4998 - val_loss: 0.5023\n",
      "Epoch 237/300\n",
      "2/2 [==============================] - 2s 493ms/step - loss: 0.4999 - val_loss: 0.5021\n",
      "Epoch 238/300\n",
      "2/2 [==============================] - 3s 848ms/step - loss: 0.4998 - val_loss: 0.5024\n",
      "Epoch 239/300\n",
      "2/2 [==============================] - 3s 755ms/step - loss: 0.4999 - val_loss: 0.5021\n",
      "Epoch 240/300\n",
      "2/2 [==============================] - 3s 851ms/step - loss: 0.4997 - val_loss: 0.5021\n",
      "Epoch 241/300\n",
      "2/2 [==============================] - 3s 792ms/step - loss: 0.4997 - val_loss: 0.5021\n",
      "Epoch 242/300\n",
      "2/2 [==============================] - 2s 525ms/step - loss: 0.4997 - val_loss: 0.5021\n",
      "Epoch 243/300\n",
      "2/2 [==============================] - 2s 495ms/step - loss: 0.4997 - val_loss: 0.5021\n",
      "Epoch 244/300\n",
      "2/2 [==============================] - 2s 496ms/step - loss: 0.4997 - val_loss: 0.5021\n",
      "Epoch 245/300\n",
      "2/2 [==============================] - 2s 499ms/step - loss: 0.4997 - val_loss: 0.5020\n",
      "Epoch 246/300\n",
      "2/2 [==============================] - 2s 502ms/step - loss: 0.4996 - val_loss: 0.5020\n",
      "Epoch 247/300\n",
      "2/2 [==============================] - 2s 512ms/step - loss: 0.4996 - val_loss: 0.5020\n",
      "Epoch 248/300\n",
      "2/2 [==============================] - 2s 524ms/step - loss: 0.4996 - val_loss: 0.5020\n",
      "Epoch 249/300\n",
      "2/2 [==============================] - 2s 561ms/step - loss: 0.4996 - val_loss: 0.5019\n",
      "Epoch 250/300\n",
      "2/2 [==============================] - 2s 592ms/step - loss: 0.4996 - val_loss: 0.5020\n",
      "Epoch 251/300\n",
      "2/2 [==============================] - 2s 546ms/step - loss: 0.4996 - val_loss: 0.5020\n",
      "Epoch 252/300\n",
      "2/2 [==============================] - 2s 584ms/step - loss: 0.4995 - val_loss: 0.5019\n",
      "Epoch 253/300\n",
      "2/2 [==============================] - 2s 567ms/step - loss: 0.4995 - val_loss: 0.5019\n",
      "Epoch 254/300\n",
      "2/2 [==============================] - 2s 624ms/step - loss: 0.4995 - val_loss: 0.5019\n",
      "Epoch 255/300\n",
      "2/2 [==============================] - 2s 539ms/step - loss: 0.4995 - val_loss: 0.5019\n",
      "Epoch 256/300\n",
      "2/2 [==============================] - 2s 590ms/step - loss: 0.4995 - val_loss: 0.5019\n",
      "Epoch 257/300\n",
      "2/2 [==============================] - 2s 524ms/step - loss: 0.4995 - val_loss: 0.5019\n",
      "Epoch 258/300\n",
      "2/2 [==============================] - 2s 532ms/step - loss: 0.4995 - val_loss: 0.5018\n",
      "Epoch 259/300\n",
      "2/2 [==============================] - 2s 546ms/step - loss: 0.4994 - val_loss: 0.5019\n",
      "Epoch 260/300\n",
      "2/2 [==============================] - 2s 599ms/step - loss: 0.4995 - val_loss: 0.5019\n",
      "Epoch 261/300\n",
      "2/2 [==============================] - 2s 535ms/step - loss: 0.4995 - val_loss: 0.5018\n",
      "Epoch 262/300\n",
      "2/2 [==============================] - 2s 571ms/step - loss: 0.4994 - val_loss: 0.5018\n",
      "Epoch 263/300\n",
      "2/2 [==============================] - 2s 534ms/step - loss: 0.4994 - val_loss: 0.5019\n",
      "Epoch 264/300\n",
      "2/2 [==============================] - 2s 520ms/step - loss: 0.4995 - val_loss: 0.5017\n",
      "Epoch 265/300\n",
      "2/2 [==============================] - 2s 584ms/step - loss: 0.4993 - val_loss: 0.5018\n",
      "Epoch 266/300\n",
      "2/2 [==============================] - 2s 544ms/step - loss: 0.4994 - val_loss: 0.5017\n",
      "Epoch 267/300\n",
      "2/2 [==============================] - 2s 564ms/step - loss: 0.4993 - val_loss: 0.5017\n",
      "Epoch 268/300\n",
      "2/2 [==============================] - 2s 534ms/step - loss: 0.4993 - val_loss: 0.5017\n",
      "Epoch 269/300\n",
      "2/2 [==============================] - 2s 562ms/step - loss: 0.4993 - val_loss: 0.5017\n",
      "Epoch 270/300\n",
      "2/2 [==============================] - 2s 642ms/step - loss: 0.4993 - val_loss: 0.5016\n",
      "Epoch 271/300\n",
      "2/2 [==============================] - 2s 567ms/step - loss: 0.4992 - val_loss: 0.5017\n",
      "Epoch 272/300\n",
      "2/2 [==============================] - 2s 569ms/step - loss: 0.4993 - val_loss: 0.5016\n",
      "Epoch 273/300\n",
      "2/2 [==============================] - 2s 541ms/step - loss: 0.4993 - val_loss: 0.5016\n",
      "Epoch 274/300\n",
      "2/2 [==============================] - 2s 547ms/step - loss: 0.4992 - val_loss: 0.5016\n",
      "Epoch 275/300\n",
      "2/2 [==============================] - 2s 561ms/step - loss: 0.4992 - val_loss: 0.5016\n",
      "Epoch 276/300\n",
      "2/2 [==============================] - 2s 578ms/step - loss: 0.4992 - val_loss: 0.5016\n",
      "Epoch 277/300\n",
      "2/2 [==============================] - 2s 565ms/step - loss: 0.4992 - val_loss: 0.5016\n",
      "Epoch 278/300\n",
      "2/2 [==============================] - 2s 546ms/step - loss: 0.4992 - val_loss: 0.5015\n",
      "Epoch 279/300\n",
      "2/2 [==============================] - 2s 580ms/step - loss: 0.4991 - val_loss: 0.5016\n",
      "Epoch 280/300\n",
      "2/2 [==============================] - 2s 569ms/step - loss: 0.4991 - val_loss: 0.5016\n",
      "Epoch 281/300\n",
      "2/2 [==============================] - 2s 555ms/step - loss: 0.4991 - val_loss: 0.5015\n",
      "Epoch 282/300\n",
      "2/2 [==============================] - 2s 564ms/step - loss: 0.4991 - val_loss: 0.5015\n",
      "Epoch 283/300\n",
      "2/2 [==============================] - 2s 553ms/step - loss: 0.4991 - val_loss: 0.5015\n",
      "Epoch 284/300\n",
      "2/2 [==============================] - 2s 553ms/step - loss: 0.4991 - val_loss: 0.5014\n",
      "Epoch 285/300\n",
      "2/2 [==============================] - 2s 558ms/step - loss: 0.4991 - val_loss: 0.5014\n",
      "Epoch 286/300\n",
      "2/2 [==============================] - 2s 565ms/step - loss: 0.4991 - val_loss: 0.5016\n",
      "Epoch 287/300\n",
      "2/2 [==============================] - 2s 578ms/step - loss: 0.4992 - val_loss: 0.5014\n",
      "Epoch 288/300\n",
      "2/2 [==============================] - 2s 554ms/step - loss: 0.4990 - val_loss: 0.5014\n",
      "Epoch 289/300\n",
      "2/2 [==============================] - 2s 573ms/step - loss: 0.4991 - val_loss: 0.5014\n",
      "Epoch 290/300\n",
      "2/2 [==============================] - 2s 549ms/step - loss: 0.4990 - val_loss: 0.5014\n",
      "Epoch 291/300\n",
      "2/2 [==============================] - 2s 560ms/step - loss: 0.4990 - val_loss: 0.5014\n",
      "Epoch 292/300\n",
      "2/2 [==============================] - 2s 530ms/step - loss: 0.4990 - val_loss: 0.5014\n",
      "Epoch 293/300\n",
      "2/2 [==============================] - 2s 511ms/step - loss: 0.4990 - val_loss: 0.5013\n",
      "Epoch 294/300\n",
      "2/2 [==============================] - 2s 493ms/step - loss: 0.4990 - val_loss: 0.5014\n",
      "Epoch 295/300\n",
      "2/2 [==============================] - 2s 521ms/step - loss: 0.4990 - val_loss: 0.5014\n",
      "Epoch 296/300\n",
      "2/2 [==============================] - 2s 558ms/step - loss: 0.4990 - val_loss: 0.5013\n",
      "Epoch 297/300\n",
      "2/2 [==============================] - 2s 619ms/step - loss: 0.4989 - val_loss: 0.5013\n",
      "Epoch 298/300\n",
      "2/2 [==============================] - 2s 570ms/step - loss: 0.4989 - val_loss: 0.5013\n",
      "Epoch 299/300\n",
      "2/2 [==============================] - 2s 575ms/step - loss: 0.4989 - val_loss: 0.5013\n",
      "Epoch 300/300\n",
      "2/2 [==============================] - 2s 505ms/step - loss: 0.4989 - val_loss: 0.5014\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "es_callback = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "history = autoencoder.fit(X_train_sc, X_train_sc, validation_split=0.2, batch_size=128, epochs=300, callbacks=[es_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 135ms/step\n",
      "3/3 [==============================] - 0s 112ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((84, 7005, 11), (84, 7005, 11))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_val = autoencoder.predict(X_val_sc)\n",
    "p_test = autoencoder.predict(X_test_sc)\n",
    "p_val.shape, p_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified rce shape: (84,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 재건 에러 계산 함수\n",
    "def reconstruction_error(x, p):\n",
    "    return np.mean(np.abs(x - p), axis=(1, -1))\n",
    "\n",
    "rce = reconstruction_error(X_test_sc, p_test)\n",
    "print(f\"Modified rce shape: {rce.shape}\")\n",
    "\n",
    "# rce와 y_val을 하나의 DataFrame으로 만듭니다.\n",
    "df = pd.DataFrame({'ReconstructionError': rce, 'Label': y_val})\n",
    "\n",
    "# 히스토그램을 그립니다.\n",
    "plt.figure()\n",
    "sns.histplot(data=df, x='ReconstructionError', hue='Label', bins=50, kde=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.9285714285714286\n",
      "F1 점수: 0.0\n",
      "[[78  0]\n",
      " [ 6  0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, ConfusionMatrixDisplay, roc_auc_score, roc_curve, confusion_matrix\n",
    "\n",
    "# 임계값 설정, 이 값은 문제에 따라 다르게 설정해야 할 수 있습니다.\n",
    "threshold = 0.5\n",
    "\n",
    "# 재구성 에러 계산\n",
    "rce = reconstruction_error(X_test_sc, p_test)\n",
    "\n",
    "# 재구성 에러가 임계값보다 큰 경우를 'Bad'(1)로, 그렇지 않은 경우를 'Good'(0)으로 분류\n",
    "p_val_bin = (rce > threshold).astype(int)\n",
    "\n",
    "# 이제 p_val_bin과 y_val의 차원이 일치하므로, 평가 메트릭을 계산할 수 있습니다.\n",
    "print('정확도:', accuracy_score(y_test, p_val_bin))\n",
    "print('F1 점수:', f1_score(y_test, p_val_bin))\n",
    "print(confusion_matrix(y_test, p_val_bin))\n",
    "plt.figure()\n",
    "ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_test, p_val_bin), display_labels=['Good', 'Bad']).plot(cmap='Blues')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
