{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.24.1.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from plotly.offline import init_notebook_mode,iplot\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.rcParams['font.family'] = 'Malgun Gothic'\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F_RunState</th>\n",
       "      <th>F_CycleTime</th>\n",
       "      <th>F_SpindleRPM1</th>\n",
       "      <th>F_SpindleTroq1</th>\n",
       "      <th>F_SpindleGearRatio1</th>\n",
       "      <th>F_ToolNum</th>\n",
       "      <th>G_ADC1</th>\n",
       "      <th>G_ADC2</th>\n",
       "      <th>G_ADC3</th>\n",
       "      <th>G_MV</th>\n",
       "      <th>G_MA</th>\n",
       "      <th>G_MActP</th>\n",
       "      <th>G_MFeq</th>\n",
       "      <th>G_MTemp</th>\n",
       "      <th>label</th>\n",
       "      <th>anomaly</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GDatetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-06-05 07:03:45</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.400000</td>\n",
       "      <td>43.700001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>222.020004</td>\n",
       "      <td>2.974</td>\n",
       "      <td>0.924</td>\n",
       "      <td>59.970001</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-05 07:03:46</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.400000</td>\n",
       "      <td>43.700001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>221.529999</td>\n",
       "      <td>2.976</td>\n",
       "      <td>0.924</td>\n",
       "      <td>59.970001</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-05 07:03:47</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.400000</td>\n",
       "      <td>43.700001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>221.529999</td>\n",
       "      <td>2.976</td>\n",
       "      <td>0.924</td>\n",
       "      <td>59.970001</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-05 07:03:48</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.400000</td>\n",
       "      <td>43.700001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>220.729996</td>\n",
       "      <td>2.976</td>\n",
       "      <td>0.920</td>\n",
       "      <td>59.959999</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-05 07:03:49</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.299999</td>\n",
       "      <td>43.700001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>221.630005</td>\n",
       "      <td>2.962</td>\n",
       "      <td>0.920</td>\n",
       "      <td>59.959999</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-01 16:27:16</th>\n",
       "      <td>2.0</td>\n",
       "      <td>10424.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>712.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>37.200001</td>\n",
       "      <td>38.900002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>220.570007</td>\n",
       "      <td>11.092</td>\n",
       "      <td>2.656</td>\n",
       "      <td>59.970001</td>\n",
       "      <td>45.0</td>\n",
       "      <td>363</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-01 16:27:17</th>\n",
       "      <td>2.0</td>\n",
       "      <td>10424.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>712.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>37.200001</td>\n",
       "      <td>38.900002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>220.669998</td>\n",
       "      <td>11.046</td>\n",
       "      <td>2.644</td>\n",
       "      <td>59.970001</td>\n",
       "      <td>45.0</td>\n",
       "      <td>363</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-01 16:27:18</th>\n",
       "      <td>2.0</td>\n",
       "      <td>10424.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>712.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>37.200001</td>\n",
       "      <td>38.900002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>220.669998</td>\n",
       "      <td>11.046</td>\n",
       "      <td>2.644</td>\n",
       "      <td>59.970001</td>\n",
       "      <td>45.0</td>\n",
       "      <td>363</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-01 16:27:19</th>\n",
       "      <td>2.0</td>\n",
       "      <td>10424.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>712.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>37.099998</td>\n",
       "      <td>38.900002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>220.960007</td>\n",
       "      <td>11.102</td>\n",
       "      <td>2.660</td>\n",
       "      <td>59.970001</td>\n",
       "      <td>45.0</td>\n",
       "      <td>363</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-01 16:27:20</th>\n",
       "      <td>2.0</td>\n",
       "      <td>10424.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>712.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>37.200001</td>\n",
       "      <td>38.900002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>220.779999</td>\n",
       "      <td>11.101</td>\n",
       "      <td>2.664</td>\n",
       "      <td>59.970001</td>\n",
       "      <td>45.0</td>\n",
       "      <td>363</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1621788 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     F_RunState  F_CycleTime  F_SpindleRPM1  F_SpindleTroq1  \\\n",
       "GDatetime                                                                     \n",
       "2023-06-05 07:03:45         2.0          0.0            0.0             0.0   \n",
       "2023-06-05 07:03:46         2.0          0.0            0.0             0.0   \n",
       "2023-06-05 07:03:47         2.0          0.0            0.0             0.0   \n",
       "2023-06-05 07:03:48         2.0          0.0            0.0             0.0   \n",
       "2023-06-05 07:03:49         2.0          0.0            0.0             0.0   \n",
       "...                         ...          ...            ...             ...   \n",
       "2023-07-01 16:27:16         2.0      10424.0            0.0             0.0   \n",
       "2023-07-01 16:27:17         2.0      10424.0            0.0             0.0   \n",
       "2023-07-01 16:27:18         2.0      10424.0            0.0             0.0   \n",
       "2023-07-01 16:27:19         2.0      10424.0            0.0             0.0   \n",
       "2023-07-01 16:27:20         2.0      10424.0            0.0             0.0   \n",
       "\n",
       "                     F_SpindleGearRatio1  F_ToolNum     G_ADC1     G_ADC2  \\\n",
       "GDatetime                                                                   \n",
       "2023-06-05 07:03:45                  0.0        0.0  24.400000  43.700001   \n",
       "2023-06-05 07:03:46                  0.0        0.0  24.400000  43.700001   \n",
       "2023-06-05 07:03:47                  0.0        0.0  24.400000  43.700001   \n",
       "2023-06-05 07:03:48                  0.0        0.0  24.400000  43.700001   \n",
       "2023-06-05 07:03:49                  0.0        0.0  24.299999  43.700001   \n",
       "...                                  ...        ...        ...        ...   \n",
       "2023-07-01 16:27:16                712.0       21.0  37.200001  38.900002   \n",
       "2023-07-01 16:27:17                712.0       21.0  37.200001  38.900002   \n",
       "2023-07-01 16:27:18                712.0       21.0  37.200001  38.900002   \n",
       "2023-07-01 16:27:19                712.0       21.0  37.099998  38.900002   \n",
       "2023-07-01 16:27:20                712.0       21.0  37.200001  38.900002   \n",
       "\n",
       "                     G_ADC3        G_MV    G_MA  G_MActP     G_MFeq  G_MTemp  \\\n",
       "GDatetime                                                                      \n",
       "2023-06-05 07:03:45     0.0  222.020004   2.974    0.924  59.970001     39.0   \n",
       "2023-06-05 07:03:46     0.0  221.529999   2.976    0.924  59.970001     39.0   \n",
       "2023-06-05 07:03:47     0.0  221.529999   2.976    0.924  59.970001     39.0   \n",
       "2023-06-05 07:03:48     0.0  220.729996   2.976    0.920  59.959999     39.0   \n",
       "2023-06-05 07:03:49     0.0  221.630005   2.962    0.920  59.959999     39.0   \n",
       "...                     ...         ...     ...      ...        ...      ...   \n",
       "2023-07-01 16:27:16     0.0  220.570007  11.092    2.656  59.970001     45.0   \n",
       "2023-07-01 16:27:17     0.0  220.669998  11.046    2.644  59.970001     45.0   \n",
       "2023-07-01 16:27:18     0.0  220.669998  11.046    2.644  59.970001     45.0   \n",
       "2023-07-01 16:27:19     0.0  220.960007  11.102    2.660  59.970001     45.0   \n",
       "2023-07-01 16:27:20     0.0  220.779999  11.101    2.664  59.970001     45.0   \n",
       "\n",
       "                     label  anomaly  \n",
       "GDatetime                            \n",
       "2023-06-05 07:03:45      0        0  \n",
       "2023-06-05 07:03:46      0        0  \n",
       "2023-06-05 07:03:47      0        0  \n",
       "2023-06-05 07:03:48      0        0  \n",
       "2023-06-05 07:03:49      0        0  \n",
       "...                    ...      ...  \n",
       "2023-07-01 16:27:16    363        0  \n",
       "2023-07-01 16:27:17    363        0  \n",
       "2023-07-01 16:27:18    363        0  \n",
       "2023-07-01 16:27:19    363        0  \n",
       "2023-07-01 16:27:20    363        0  \n",
       "\n",
       "[1621788 rows x 16 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('selected_data.csv')\n",
    "data['GDatetime'] = pd.to_datetime(data['GDatetime'])\n",
    "data.set_index('GDatetime', inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F_RunState</th>\n",
       "      <th>F_CycleTime</th>\n",
       "      <th>F_SpindleRPM1</th>\n",
       "      <th>F_SpindleTroq1</th>\n",
       "      <th>F_SpindleGearRatio1</th>\n",
       "      <th>F_ToolNum</th>\n",
       "      <th>G_ADC1</th>\n",
       "      <th>G_ADC2</th>\n",
       "      <th>G_ADC3</th>\n",
       "      <th>G_MV</th>\n",
       "      <th>G_MA</th>\n",
       "      <th>G_MActP</th>\n",
       "      <th>G_MFeq</th>\n",
       "      <th>G_MTemp</th>\n",
       "      <th>label</th>\n",
       "      <th>anomaly</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GDatetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-06-07 15:28:03</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>712.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>34.700001</td>\n",
       "      <td>33.900002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>215.940002</td>\n",
       "      <td>10.889</td>\n",
       "      <td>2.644</td>\n",
       "      <td>59.990002</td>\n",
       "      <td>42.0</td>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-07 15:28:04</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>712.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>34.700001</td>\n",
       "      <td>33.900002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>216.070007</td>\n",
       "      <td>10.927</td>\n",
       "      <td>2.648</td>\n",
       "      <td>59.990002</td>\n",
       "      <td>42.0</td>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-07 15:28:05</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>712.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>34.700001</td>\n",
       "      <td>33.900002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>215.729996</td>\n",
       "      <td>13.332</td>\n",
       "      <td>2.956</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>43.0</td>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-07 15:28:06</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>712.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>34.799999</td>\n",
       "      <td>33.799999</td>\n",
       "      <td>0.0</td>\n",
       "      <td>216.529999</td>\n",
       "      <td>13.121</td>\n",
       "      <td>2.912</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>43.0</td>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-07 15:28:07</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>712.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>34.799999</td>\n",
       "      <td>33.799999</td>\n",
       "      <td>0.0</td>\n",
       "      <td>216.529999</td>\n",
       "      <td>13.121</td>\n",
       "      <td>2.912</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>43.0</td>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-01 13:04:23</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4550.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>712.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>35.799999</td>\n",
       "      <td>41.299999</td>\n",
       "      <td>0.0</td>\n",
       "      <td>218.589996</td>\n",
       "      <td>12.695</td>\n",
       "      <td>2.772</td>\n",
       "      <td>60.009998</td>\n",
       "      <td>45.0</td>\n",
       "      <td>360</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-01 13:04:24</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4552.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>712.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>35.799999</td>\n",
       "      <td>41.299999</td>\n",
       "      <td>0.0</td>\n",
       "      <td>218.589996</td>\n",
       "      <td>12.695</td>\n",
       "      <td>2.772</td>\n",
       "      <td>60.009998</td>\n",
       "      <td>45.0</td>\n",
       "      <td>360</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-01 13:04:25</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4553.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>712.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>35.799999</td>\n",
       "      <td>41.299999</td>\n",
       "      <td>0.0</td>\n",
       "      <td>217.669998</td>\n",
       "      <td>12.662</td>\n",
       "      <td>2.792</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>45.0</td>\n",
       "      <td>360</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-01 13:04:26</th>\n",
       "      <td>2.0</td>\n",
       "      <td>4554.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>712.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>35.799999</td>\n",
       "      <td>41.299999</td>\n",
       "      <td>0.0</td>\n",
       "      <td>217.949997</td>\n",
       "      <td>12.732</td>\n",
       "      <td>2.784</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>45.0</td>\n",
       "      <td>360</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-01 13:04:27</th>\n",
       "      <td>2.0</td>\n",
       "      <td>4554.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>712.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>35.799999</td>\n",
       "      <td>41.299999</td>\n",
       "      <td>0.0</td>\n",
       "      <td>217.949997</td>\n",
       "      <td>12.732</td>\n",
       "      <td>2.784</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>45.0</td>\n",
       "      <td>360</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68936 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     F_RunState  F_CycleTime  F_SpindleRPM1  F_SpindleTroq1  \\\n",
       "GDatetime                                                                     \n",
       "2023-06-07 15:28:03         2.0          0.0            0.0             0.0   \n",
       "2023-06-07 15:28:04         3.0          0.0            0.0             0.0   \n",
       "2023-06-07 15:28:05         3.0          0.0            0.0             0.0   \n",
       "2023-06-07 15:28:06         3.0          0.0            0.0             0.0   \n",
       "2023-06-07 15:28:07         3.0          0.0            0.0             0.0   \n",
       "...                         ...          ...            ...             ...   \n",
       "2023-07-01 13:04:23         1.0       4550.0            0.0             0.0   \n",
       "2023-07-01 13:04:24         1.0       4552.0            0.0             0.0   \n",
       "2023-07-01 13:04:25         1.0       4553.0            0.0             0.0   \n",
       "2023-07-01 13:04:26         2.0       4554.0            0.0             0.0   \n",
       "2023-07-01 13:04:27         2.0       4554.0            0.0             0.0   \n",
       "\n",
       "                     F_SpindleGearRatio1  F_ToolNum     G_ADC1     G_ADC2  \\\n",
       "GDatetime                                                                   \n",
       "2023-06-07 15:28:03                712.0       21.0  34.700001  33.900002   \n",
       "2023-06-07 15:28:04                712.0       21.0  34.700001  33.900002   \n",
       "2023-06-07 15:28:05                712.0       21.0  34.700001  33.900002   \n",
       "2023-06-07 15:28:06                712.0       21.0  34.799999  33.799999   \n",
       "2023-06-07 15:28:07                712.0       21.0  34.799999  33.799999   \n",
       "...                                  ...        ...        ...        ...   \n",
       "2023-07-01 13:04:23                712.0       21.0  35.799999  41.299999   \n",
       "2023-07-01 13:04:24                712.0       21.0  35.799999  41.299999   \n",
       "2023-07-01 13:04:25                712.0       21.0  35.799999  41.299999   \n",
       "2023-07-01 13:04:26                712.0       21.0  35.799999  41.299999   \n",
       "2023-07-01 13:04:27                712.0       21.0  35.799999  41.299999   \n",
       "\n",
       "                     G_ADC3        G_MV    G_MA  G_MActP     G_MFeq  G_MTemp  \\\n",
       "GDatetime                                                                      \n",
       "2023-06-07 15:28:03     0.0  215.940002  10.889    2.644  59.990002     42.0   \n",
       "2023-06-07 15:28:04     0.0  216.070007  10.927    2.648  59.990002     42.0   \n",
       "2023-06-07 15:28:05     0.0  215.729996  13.332    2.956  60.000000     43.0   \n",
       "2023-06-07 15:28:06     0.0  216.529999  13.121    2.912  60.000000     43.0   \n",
       "2023-06-07 15:28:07     0.0  216.529999  13.121    2.912  60.000000     43.0   \n",
       "...                     ...         ...     ...      ...        ...      ...   \n",
       "2023-07-01 13:04:23     0.0  218.589996  12.695    2.772  60.009998     45.0   \n",
       "2023-07-01 13:04:24     0.0  218.589996  12.695    2.772  60.009998     45.0   \n",
       "2023-07-01 13:04:25     0.0  217.669998  12.662    2.792  60.000000     45.0   \n",
       "2023-07-01 13:04:26     0.0  217.949997  12.732    2.784  60.000000     45.0   \n",
       "2023-07-01 13:04:27     0.0  217.949997  12.732    2.784  60.000000     45.0   \n",
       "\n",
       "                     label  anomaly  \n",
       "GDatetime                            \n",
       "2023-06-07 15:28:03     43        1  \n",
       "2023-06-07 15:28:04     43        1  \n",
       "2023-06-07 15:28:05     43        1  \n",
       "2023-06-07 15:28:06     43        1  \n",
       "2023-06-07 15:28:07     43        1  \n",
       "...                    ...      ...  \n",
       "2023-07-01 13:04:23    360        1  \n",
       "2023-07-01 13:04:24    360        1  \n",
       "2023-07-01 13:04:25    360        1  \n",
       "2023-07-01 13:04:26    360        1  \n",
       "2023-07-01 13:04:27    360        1  \n",
       "\n",
       "[68936 rows x 16 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['anomaly'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_cycle_periods(data, threshold_low=100, threshold_high=10000, sub_cycle_threshold=1000, sub_cycle_length_minutes=50):\n",
    "    cycle_starts = []\n",
    "    cycle_ends = []\n",
    "    in_cycle = False\n",
    "    in_high = False\n",
    "    in_sub_cycle = False\n",
    "    label = -1 # 주기 라벨 (0부터 시작)\n",
    "    \n",
    "    # label 열을 추가하고 -1로 초기화\n",
    "    data['label'] = 0\n",
    "    sub_cycle_start_time = None\n",
    "    sub_cycle_length = 0\n",
    "    \n",
    "    for i in range(len(data) - 1):\n",
    "        value = data['F_CycleTime'].iloc[i]\n",
    "        next_value = data['F_CycleTime'].iloc[i + 1]\n",
    "        \n",
    "        # 주기 시작 지점 찾기 (0 ~ 100 근처에서 올라가기 시작)\n",
    "        if not in_cycle and value <= threshold_low:\n",
    "            in_cycle = True\n",
    "            label += 1 # 새로운 주기 시작, 라벨 증가\n",
    "            cycle_starts.append(data.index[i]) # datetime 인덱스 사용\n",
    "        \n",
    "        # 주기 내부라면 현재 라벨로 표시\n",
    "        if in_cycle:\n",
    "            data['label'].iloc[i] = label\n",
    "\n",
    "        # 1만 근처의 값에 도달\n",
    "        if in_cycle and not in_high and value >= threshold_high:\n",
    "            in_high = True\n",
    "\n",
    "        # 1천 근처의 값에 도달\n",
    "        if in_cycle and not in_high and value >= sub_cycle_threshold:\n",
    "            in_sub_cycle = True\n",
    "            if sub_cycle_start_time is None:\n",
    "                sub_cycle_start_time = data.index[i]\n",
    "\n",
    "        # 하위 주기의 끝 지점 찾기\n",
    "        if in_sub_cycle and next_value <= threshold_low:\n",
    "            sub_cycle_length += (data.index[i] - sub_cycle_start_time).seconds / 60\n",
    "            sub_cycle_start_time = None\n",
    "            if sub_cycle_length >= sub_cycle_length_minutes:\n",
    "                in_high = True\n",
    "            in_sub_cycle = False\n",
    "            \n",
    "        # 주기 끝 지점 찾기\n",
    "        if in_high and next_value <= threshold_low:\n",
    "            in_high = False\n",
    "            in_cycle = False\n",
    "            cycle_ends.append(data.index[i]) # datetime 인덱스 사용\n",
    "            sub_cycle_length = 0\n",
    "            \n",
    "    # 마지막 주기의 끝 지점 처리\n",
    "    if in_cycle:\n",
    "        cycle_ends.append(data.index[-1])\n",
    "\n",
    "    return cycle_starts, cycle_ends\n",
    "\n",
    "\n",
    "def remove_constant_values(data, lower_bound=1000, upper_bound=12000, duration_minutes=20):\n",
    "    constant_value = None\n",
    "    constant_start_time = None\n",
    "    rows_to_drop = []\n",
    "\n",
    "    for i in range(len(data) - 1):\n",
    "        value = data['F_CycleTime'].iloc[i]\n",
    "\n",
    "        # 값이 범위 내에 있고 이전 값과 동일한 경우\n",
    "        if lower_bound <= value < upper_bound and value == constant_value:\n",
    "            if constant_start_time is None:\n",
    "                constant_start_time = data.index[i]\n",
    "            # 지속 시간이 20분 이상인 경우\n",
    "            if (data.index[i] - constant_start_time).seconds / 60 >= duration_minutes:\n",
    "                rows_to_drop.append(data.index[i])\n",
    "        else:\n",
    "            constant_value = value\n",
    "            constant_start_time = None\n",
    "\n",
    "    # 행 삭제\n",
    "    data.drop(rows_to_drop, inplace=True)\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "selected_data = data\n",
    "\n",
    "selected_data = remove_constant_values(selected_data)\n",
    "\n",
    "# 중복된 인덱스를 가진 행의 평균값으로 병합\n",
    "selected_data = selected_data.groupby(selected_data.index).mean()\n",
    "\n",
    "cycle_starts, cycle_ends = find_cycle_periods(selected_data)\n",
    "\n",
    "selected_data = selected_data.iloc[:-1]\n",
    "selected_data['anomaly'] = 0\n",
    "specific_labels = [43, 188, 243, 256, 258, 270, 291, 295, 325, 340, 349, 360]\n",
    "selected_data.loc[selected_data['label'].isin(specific_labels), 'anomaly'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((364, 7005, 11), (364,))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 최대 샘플 길이를 정의 (선택적)\n",
    "max_length = max(len(selected_data.loc[start:end]) for start, end in zip(cycle_starts, cycle_ends))\n",
    "\n",
    "def create_3d_array(data, cycle_starts, cycle_ends, max_length=None, \n",
    "                    feature_columns=['F_RunState', 'F_ToolNum', 'F_CycleTime', 'F_SpindleRPM1', 'F_SpindleTroq1', 'G_ADC1', 'G_ADC2', 'G_ADC3', 'G_MV', 'G_MActP', 'G_MTemp', 'anomaly']):\n",
    "    # 주기별 샘플을 저장할 리스트\n",
    "    samples = []\n",
    "\n",
    "    # 각 주기를 샘플로 변환\n",
    "    for start, end in zip(cycle_starts, cycle_ends):\n",
    "        sample = data.loc[start:end][feature_columns].values\n",
    "        \n",
    "        # 샘플 길이 통일 (선택적)\n",
    "        if max_length:\n",
    "            if len(sample) > max_length:\n",
    "                sample = sample[:max_length]\n",
    "            elif len(sample) < max_length:\n",
    "                padding = np.zeros((max_length - len(sample), len(feature_columns)))\n",
    "                sample = np.vstack((sample, padding))\n",
    "        \n",
    "        samples.append(sample)\n",
    "\n",
    "    # 샘플들을 3차원 배열로 쌓기\n",
    "    samples_array = np.stack(samples)\n",
    "    \n",
    "    return samples_array\n",
    "\n",
    "# 3차원 배열 생성\n",
    "samples_array = create_3d_array(selected_data, cycle_starts, cycle_ends, max_length=max_length)\n",
    "\n",
    "# 결과의 형태 출력\n",
    "X = samples_array[:,:,:-1]\n",
    "y = samples_array[:,0,11]\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_val = X[:280]\n",
    "y_train_val = y[:280]\n",
    "X_test = X[280:]\n",
    "y_test = y[280:]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.3, random_state=42)\n",
    "\n",
    "std = StandardScaler()\n",
    "mm = MinMaxScaler()\n",
    "\n",
    "# 초기화\n",
    "X_train_sc = []\n",
    "X_val_sc = []\n",
    "X_test_sc = []\n",
    "\n",
    "# 학습 데이터 변환\n",
    "for sample in X_train:\n",
    "    mm_sample = mm.fit_transform(sample[:, :2])\n",
    "    std_sample = std.fit_transform(sample[:, 2:])\n",
    "    sc_sample = np.concatenate((mm_sample, std_sample), axis=1)\n",
    "    X_train_sc.append(sc_sample)\n",
    "\n",
    "# 검증 데이터 변환\n",
    "for sample in X_val:\n",
    "    mm_sample = mm.transform(sample[:, :2])\n",
    "    std_sample = std.transform(sample[:, 2:])\n",
    "    sc_sample = np.concatenate((mm_sample, std_sample), axis=1)\n",
    "    X_val_sc.append(sc_sample)\n",
    "\n",
    "# 테스트 데이터 변환\n",
    "for sample in X_test:\n",
    "    mm_sample = mm.transform(sample[:, :2])\n",
    "    std_sample = std.transform(sample[:, 2:])\n",
    "    sc_sample = np.concatenate((mm_sample, std_sample), axis=1)\n",
    "    X_test_sc.append(sc_sample)\n",
    "\n",
    "# NumPy 배열로 변환\n",
    "X_train_sc = np.array(X_train_sc)\n",
    "X_val_sc = np.array(X_val_sc)\n",
    "X_test_sc = np.array(X_test_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(196, 7005, 11) (196,)\n",
      "(84, 7005, 11) (84,)\n",
      "(84, 7005, 11) (84,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_sc.shape, y_train.shape)\n",
    "print(X_val_sc.shape, y_val.shape)\n",
    "print(X_test_sc.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 7005, 11)]        0         \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 3503, 64)          7104      \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 3503, 64)          256       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " activation (Activation)     (None, 3503, 64)          0         \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 3503, 128)         57472     \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 3503, 128)         512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 3503, 128)         0         \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 3503, 128)         82048     \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 3503, 128)         512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 3503, 128)         0         \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, 3503, 256)         98560     \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 3503, 256)         1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 3503, 256)         0         \n",
      "                                                                 \n",
      " global_max_pooling1d (Glob  (None, 256)               0         \n",
      " alMaxPooling1D)                                                 \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                16448     \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 64)                256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 64)                0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_5 (Bat  (None, 64)                256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 64)                0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " batch_normalization_6 (Bat  (None, 32)                128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 32)                0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 270849 (1.03 MB)\n",
      "Trainable params: 269377 (1.03 MB)\n",
      "Non-trainable params: 1472 (5.75 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "seq_len = 7005\n",
    "n_channels = 11\n",
    "\n",
    "inputs = layers.Input(shape=(seq_len, n_channels))\n",
    "\n",
    "# Body\n",
    "x = layers.Conv1D(64, 10, padding='same', strides=2)(inputs)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Activation('relu')(x)\n",
    "\n",
    "x = layers.Conv1D(128, 7, padding='same', strides=1)(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Activation('relu')(x)\n",
    "\n",
    "x = layers.Conv1D(128, 5, padding='same', strides=1)(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Activation('relu')(x)\n",
    "\n",
    "x = layers.Conv1D(256, 3, padding='same', strides=1)(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Activation('relu')(x)\n",
    "\n",
    "# Neck\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "\n",
    "# Header\n",
    "x = layers.Dense(64)(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Activation('relu')(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "\n",
    "x = layers.Dense(64)(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Activation('relu')(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "\n",
    "x = layers.Dense(32)(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Activation('relu')(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "\n",
    "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "2/2 [==============================] - 11s 2s/step - loss: 1.0641 - acc: 0.3013 - val_loss: 0.8268 - val_acc: 0.0750\n",
      "Epoch 2/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.9711 - acc: 0.2885 - val_loss: 0.7698 - val_acc: 0.0750\n",
      "Epoch 3/300\n",
      "2/2 [==============================] - 8s 2s/step - loss: 0.9408 - acc: 0.3205 - val_loss: 0.7023 - val_acc: 0.2750\n",
      "Epoch 4/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.9078 - acc: 0.3782 - val_loss: 0.6735 - val_acc: 0.8750\n",
      "Epoch 5/300\n",
      "2/2 [==============================] - 8s 2s/step - loss: 0.8230 - acc: 0.4679 - val_loss: 0.6852 - val_acc: 0.7000\n",
      "Epoch 6/300\n",
      "2/2 [==============================] - 8s 2s/step - loss: 0.9093 - acc: 0.3141 - val_loss: 0.6881 - val_acc: 0.6500\n",
      "Epoch 7/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.7555 - acc: 0.4936 - val_loss: 0.6827 - val_acc: 0.7750\n",
      "Epoch 8/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.9022 - acc: 0.4487 - val_loss: 0.6852 - val_acc: 0.7500\n",
      "Epoch 9/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.7835 - acc: 0.4551 - val_loss: 0.6891 - val_acc: 0.6000\n",
      "Epoch 10/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.8319 - acc: 0.4231 - val_loss: 0.6919 - val_acc: 0.6000\n",
      "Epoch 11/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.7423 - acc: 0.5192 - val_loss: 0.6907 - val_acc: 0.5750\n",
      "Epoch 12/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.7579 - acc: 0.5128 - val_loss: 0.6746 - val_acc: 0.8000\n",
      "Epoch 13/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.6155 - acc: 0.6795 - val_loss: 0.6726 - val_acc: 0.8250\n",
      "Epoch 14/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.7315 - acc: 0.5385 - val_loss: 0.6730 - val_acc: 0.8250\n",
      "Epoch 15/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.6831 - acc: 0.5769 - val_loss: 0.6648 - val_acc: 0.8500\n",
      "Epoch 16/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.5989 - acc: 0.7500 - val_loss: 0.6632 - val_acc: 0.8500\n",
      "Epoch 17/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.6562 - acc: 0.6603 - val_loss: 0.6633 - val_acc: 0.7750\n",
      "Epoch 18/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.6745 - acc: 0.6090 - val_loss: 0.6563 - val_acc: 0.8000\n",
      "Epoch 19/300\n",
      "2/2 [==============================] - 7s 1s/step - loss: 0.6155 - acc: 0.6603 - val_loss: 0.6589 - val_acc: 0.8000\n",
      "Epoch 20/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.5633 - acc: 0.7115 - val_loss: 0.6547 - val_acc: 0.8500\n",
      "Epoch 21/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.5991 - acc: 0.7244 - val_loss: 0.6455 - val_acc: 0.9250\n",
      "Epoch 22/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.5631 - acc: 0.7949 - val_loss: 0.6434 - val_acc: 0.9250\n",
      "Epoch 23/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.5509 - acc: 0.7628 - val_loss: 0.6399 - val_acc: 0.9250\n",
      "Epoch 24/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.5188 - acc: 0.8462 - val_loss: 0.6340 - val_acc: 0.9250\n",
      "Epoch 25/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.5420 - acc: 0.8526 - val_loss: 0.6287 - val_acc: 0.9250\n",
      "Epoch 26/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.5560 - acc: 0.7821 - val_loss: 0.6171 - val_acc: 0.9250\n",
      "Epoch 27/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.5400 - acc: 0.8013 - val_loss: 0.6078 - val_acc: 0.9250\n",
      "Epoch 28/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.5320 - acc: 0.7821 - val_loss: 0.6029 - val_acc: 0.9250\n",
      "Epoch 29/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.4795 - acc: 0.8718 - val_loss: 0.5990 - val_acc: 0.9250\n",
      "Epoch 30/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.4678 - acc: 0.8654 - val_loss: 0.5934 - val_acc: 0.9250\n",
      "Epoch 31/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.4604 - acc: 0.9038 - val_loss: 0.5797 - val_acc: 0.9250\n",
      "Epoch 32/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.4623 - acc: 0.8910 - val_loss: 0.5654 - val_acc: 0.9250\n",
      "Epoch 33/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.4123 - acc: 0.9359 - val_loss: 0.5498 - val_acc: 0.9250\n",
      "Epoch 34/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.3763 - acc: 0.9359 - val_loss: 0.5351 - val_acc: 0.9250\n",
      "Epoch 35/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.3431 - acc: 0.9551 - val_loss: 0.5252 - val_acc: 0.9250\n",
      "Epoch 36/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.3551 - acc: 0.9295 - val_loss: 0.5154 - val_acc: 0.9250\n",
      "Epoch 37/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.3887 - acc: 0.9295 - val_loss: 0.5055 - val_acc: 0.9250\n",
      "Epoch 38/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.3942 - acc: 0.9487 - val_loss: 0.5007 - val_acc: 0.9250\n",
      "Epoch 39/300\n",
      "2/2 [==============================] - 7s 1s/step - loss: 0.3590 - acc: 0.9423 - val_loss: 0.4967 - val_acc: 0.9250\n",
      "Epoch 40/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.4115 - acc: 0.8654 - val_loss: 0.4933 - val_acc: 0.9250\n",
      "Epoch 41/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.3414 - acc: 0.9359 - val_loss: 0.4886 - val_acc: 0.9250\n",
      "Epoch 42/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.3575 - acc: 0.9487 - val_loss: 0.4809 - val_acc: 0.9250\n",
      "Epoch 43/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.3296 - acc: 0.9487 - val_loss: 0.4739 - val_acc: 0.9250\n",
      "Epoch 44/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.3542 - acc: 0.9551 - val_loss: 0.4704 - val_acc: 0.9250\n",
      "Epoch 45/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.3310 - acc: 0.9423 - val_loss: 0.4672 - val_acc: 0.9250\n",
      "Epoch 46/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.3520 - acc: 0.9295 - val_loss: 0.4631 - val_acc: 0.9250\n",
      "Epoch 47/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.3156 - acc: 0.9808 - val_loss: 0.4549 - val_acc: 0.9250\n",
      "Epoch 48/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.3088 - acc: 0.9423 - val_loss: 0.4439 - val_acc: 0.9250\n",
      "Epoch 49/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.2789 - acc: 0.9808 - val_loss: 0.4333 - val_acc: 0.9250\n",
      "Epoch 50/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.3064 - acc: 0.9423 - val_loss: 0.4264 - val_acc: 0.9250\n",
      "Epoch 51/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.2554 - acc: 0.9872 - val_loss: 0.4198 - val_acc: 0.9250\n",
      "Epoch 52/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.2757 - acc: 0.9679 - val_loss: 0.4117 - val_acc: 0.9250\n",
      "Epoch 53/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.2923 - acc: 0.9679 - val_loss: 0.4037 - val_acc: 0.9250\n",
      "Epoch 54/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.3204 - acc: 0.9615 - val_loss: 0.3977 - val_acc: 0.9250\n",
      "Epoch 55/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.2383 - acc: 1.0000 - val_loss: 0.3915 - val_acc: 0.9250\n",
      "Epoch 56/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.2544 - acc: 1.0000 - val_loss: 0.3858 - val_acc: 0.9250\n",
      "Epoch 57/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.2747 - acc: 0.9744 - val_loss: 0.3800 - val_acc: 0.9250\n",
      "Epoch 58/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.2514 - acc: 0.9679 - val_loss: 0.3734 - val_acc: 0.9250\n",
      "Epoch 59/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.2769 - acc: 0.9744 - val_loss: 0.3681 - val_acc: 0.9250\n",
      "Epoch 60/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.2391 - acc: 0.9872 - val_loss: 0.3622 - val_acc: 0.9250\n",
      "Epoch 61/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.2787 - acc: 0.9744 - val_loss: 0.3592 - val_acc: 0.9250\n",
      "Epoch 62/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.2469 - acc: 0.9872 - val_loss: 0.3574 - val_acc: 0.9250\n",
      "Epoch 63/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.2379 - acc: 0.9872 - val_loss: 0.3543 - val_acc: 0.9250\n",
      "Epoch 64/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.2406 - acc: 0.9808 - val_loss: 0.3539 - val_acc: 0.9250\n",
      "Epoch 65/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.2177 - acc: 0.9936 - val_loss: 0.3538 - val_acc: 0.9250\n",
      "Epoch 66/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.2388 - acc: 0.9808 - val_loss: 0.3531 - val_acc: 0.9250\n",
      "Epoch 67/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.2393 - acc: 0.9872 - val_loss: 0.3504 - val_acc: 0.9250\n",
      "Epoch 68/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.2157 - acc: 0.9808 - val_loss: 0.3485 - val_acc: 0.9250\n",
      "Epoch 69/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.2193 - acc: 0.9744 - val_loss: 0.3480 - val_acc: 0.9250\n",
      "Epoch 70/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.2089 - acc: 0.9936 - val_loss: 0.3475 - val_acc: 0.9250\n",
      "Epoch 71/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.1973 - acc: 0.9936 - val_loss: 0.3450 - val_acc: 0.9250\n",
      "Epoch 72/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.2008 - acc: 0.9808 - val_loss: 0.3415 - val_acc: 0.9250\n",
      "Epoch 73/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.1633 - acc: 1.0000 - val_loss: 0.3382 - val_acc: 0.9250\n",
      "Epoch 74/300\n",
      "2/2 [==============================] - 7s 1s/step - loss: 0.1908 - acc: 0.9808 - val_loss: 0.3388 - val_acc: 0.9250\n",
      "Epoch 75/300\n",
      "2/2 [==============================] - 7s 1s/step - loss: 0.2008 - acc: 0.9808 - val_loss: 0.3400 - val_acc: 0.9250\n",
      "Epoch 76/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.1811 - acc: 0.9872 - val_loss: 0.3407 - val_acc: 0.9250\n",
      "Epoch 77/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.1906 - acc: 0.9808 - val_loss: 0.3406 - val_acc: 0.9250\n",
      "Epoch 78/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.1742 - acc: 1.0000 - val_loss: 0.3398 - val_acc: 0.9250\n",
      "Epoch 79/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.1756 - acc: 1.0000 - val_loss: 0.3378 - val_acc: 0.9250\n",
      "Epoch 80/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.1977 - acc: 0.9744 - val_loss: 0.3339 - val_acc: 0.9250\n",
      "Epoch 81/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.1826 - acc: 0.9744 - val_loss: 0.3314 - val_acc: 0.9250\n",
      "Epoch 82/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.1529 - acc: 0.9936 - val_loss: 0.3301 - val_acc: 0.9250\n",
      "Epoch 83/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.1464 - acc: 0.9872 - val_loss: 0.3266 - val_acc: 0.9250\n",
      "Epoch 84/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.2128 - acc: 0.9744 - val_loss: 0.3201 - val_acc: 0.9250\n",
      "Epoch 85/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.1537 - acc: 0.9872 - val_loss: 0.3143 - val_acc: 0.9250\n",
      "Epoch 86/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.1470 - acc: 0.9872 - val_loss: 0.3102 - val_acc: 0.9250\n",
      "Epoch 87/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.1373 - acc: 0.9936 - val_loss: 0.3077 - val_acc: 0.9250\n",
      "Epoch 88/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.1657 - acc: 0.9936 - val_loss: 0.3057 - val_acc: 0.9250\n",
      "Epoch 89/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.1514 - acc: 0.9936 - val_loss: 0.3046 - val_acc: 0.9250\n",
      "Epoch 90/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.1566 - acc: 1.0000 - val_loss: 0.3045 - val_acc: 0.9250\n",
      "Epoch 91/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.1503 - acc: 0.9936 - val_loss: 0.3045 - val_acc: 0.9250\n",
      "Epoch 92/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.1435 - acc: 0.9936 - val_loss: 0.3055 - val_acc: 0.9250\n",
      "Epoch 93/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.1466 - acc: 0.9936 - val_loss: 0.3050 - val_acc: 0.9250\n",
      "Epoch 94/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.1244 - acc: 1.0000 - val_loss: 0.3039 - val_acc: 0.9250\n",
      "Epoch 95/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.1393 - acc: 0.9936 - val_loss: 0.3020 - val_acc: 0.9250\n",
      "Epoch 96/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.1353 - acc: 0.9936 - val_loss: 0.3000 - val_acc: 0.9250\n",
      "Epoch 97/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.1272 - acc: 1.0000 - val_loss: 0.2979 - val_acc: 0.9250\n",
      "Epoch 98/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.1445 - acc: 0.9872 - val_loss: 0.2943 - val_acc: 0.9250\n",
      "Epoch 99/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.1196 - acc: 1.0000 - val_loss: 0.2907 - val_acc: 0.9250\n",
      "Epoch 100/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.1150 - acc: 1.0000 - val_loss: 0.2871 - val_acc: 0.9250\n",
      "Epoch 101/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.1127 - acc: 0.9936 - val_loss: 0.2846 - val_acc: 0.9250\n",
      "Epoch 102/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.1148 - acc: 1.0000 - val_loss: 0.2833 - val_acc: 0.9250\n",
      "Epoch 103/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.1395 - acc: 0.9872 - val_loss: 0.2822 - val_acc: 0.9250\n",
      "Epoch 104/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.1154 - acc: 0.9936 - val_loss: 0.2815 - val_acc: 0.9250\n",
      "Epoch 105/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.1174 - acc: 0.9936 - val_loss: 0.2816 - val_acc: 0.9250\n",
      "Epoch 106/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.1362 - acc: 0.9808 - val_loss: 0.2809 - val_acc: 0.9250\n",
      "Epoch 107/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.1133 - acc: 1.0000 - val_loss: 0.2799 - val_acc: 0.9250\n",
      "Epoch 108/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.1267 - acc: 0.9936 - val_loss: 0.2778 - val_acc: 0.9250\n",
      "Epoch 109/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.1155 - acc: 1.0000 - val_loss: 0.2763 - val_acc: 0.9250\n",
      "Epoch 110/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.1108 - acc: 0.9872 - val_loss: 0.2749 - val_acc: 0.9250\n",
      "Epoch 111/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.1200 - acc: 1.0000 - val_loss: 0.2739 - val_acc: 0.9250\n",
      "Epoch 112/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.1014 - acc: 0.9936 - val_loss: 0.2729 - val_acc: 0.9250\n",
      "Epoch 113/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.1010 - acc: 0.9872 - val_loss: 0.2720 - val_acc: 0.9250\n",
      "Epoch 114/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.0886 - acc: 0.9936 - val_loss: 0.2713 - val_acc: 0.9250\n",
      "Epoch 115/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.1070 - acc: 0.9936 - val_loss: 0.2705 - val_acc: 0.9250\n",
      "Epoch 116/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.0851 - acc: 1.0000 - val_loss: 0.2700 - val_acc: 0.9250\n",
      "Epoch 117/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.1125 - acc: 0.9936 - val_loss: 0.2699 - val_acc: 0.9250\n",
      "Epoch 118/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.0852 - acc: 1.0000 - val_loss: 0.2693 - val_acc: 0.9250\n",
      "Epoch 119/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.0903 - acc: 1.0000 - val_loss: 0.2690 - val_acc: 0.9250\n",
      "Epoch 120/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.0972 - acc: 1.0000 - val_loss: 0.2689 - val_acc: 0.9250\n",
      "Epoch 121/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.0952 - acc: 1.0000 - val_loss: 0.2686 - val_acc: 0.9250\n",
      "Epoch 122/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.0889 - acc: 1.0000 - val_loss: 0.2689 - val_acc: 0.9250\n",
      "Epoch 123/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.1021 - acc: 0.9872 - val_loss: 0.2688 - val_acc: 0.9250\n",
      "Epoch 124/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.0817 - acc: 1.0000 - val_loss: 0.2691 - val_acc: 0.9250\n",
      "Epoch 125/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.0908 - acc: 0.9872 - val_loss: 0.2685 - val_acc: 0.9250\n",
      "Epoch 126/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.1009 - acc: 0.9872 - val_loss: 0.2669 - val_acc: 0.9250\n",
      "Epoch 127/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.1045 - acc: 0.9872 - val_loss: 0.2658 - val_acc: 0.9250\n",
      "Epoch 128/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.0795 - acc: 1.0000 - val_loss: 0.2645 - val_acc: 0.9250\n",
      "Epoch 129/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.0759 - acc: 1.0000 - val_loss: 0.2629 - val_acc: 0.9250\n",
      "Epoch 130/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.0749 - acc: 0.9936 - val_loss: 0.2614 - val_acc: 0.9250\n",
      "Epoch 131/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.0890 - acc: 1.0000 - val_loss: 0.2601 - val_acc: 0.9250\n",
      "Epoch 132/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.0872 - acc: 0.9936 - val_loss: 0.2593 - val_acc: 0.9250\n",
      "Epoch 133/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.0807 - acc: 0.9872 - val_loss: 0.2591 - val_acc: 0.9250\n",
      "Epoch 134/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.0787 - acc: 1.0000 - val_loss: 0.2594 - val_acc: 0.9250\n",
      "Epoch 135/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.0833 - acc: 1.0000 - val_loss: 0.2596 - val_acc: 0.9250\n",
      "Epoch 136/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.0999 - acc: 0.9936 - val_loss: 0.2593 - val_acc: 0.9250\n",
      "Epoch 137/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.0735 - acc: 0.9936 - val_loss: 0.2583 - val_acc: 0.9250\n",
      "Epoch 138/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.0846 - acc: 1.0000 - val_loss: 0.2574 - val_acc: 0.9250\n",
      "Epoch 139/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.0735 - acc: 1.0000 - val_loss: 0.2571 - val_acc: 0.9250\n",
      "Epoch 140/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.0771 - acc: 1.0000 - val_loss: 0.2572 - val_acc: 0.9250\n",
      "Epoch 141/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.0824 - acc: 1.0000 - val_loss: 0.2571 - val_acc: 0.9250\n",
      "Epoch 142/300\n",
      "2/2 [==============================] - 7s 1s/step - loss: 0.0586 - acc: 1.0000 - val_loss: 0.2566 - val_acc: 0.9250\n",
      "Epoch 143/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.0754 - acc: 1.0000 - val_loss: 0.2566 - val_acc: 0.9250\n",
      "Epoch 144/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.0767 - acc: 1.0000 - val_loss: 0.2561 - val_acc: 0.9250\n",
      "Epoch 145/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.0705 - acc: 0.9936 - val_loss: 0.2555 - val_acc: 0.9250\n",
      "Epoch 146/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.0624 - acc: 1.0000 - val_loss: 0.2558 - val_acc: 0.9250\n",
      "Epoch 147/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.0759 - acc: 0.9936 - val_loss: 0.2566 - val_acc: 0.9250\n",
      "Epoch 148/300\n",
      "2/2 [==============================] - 7s 1s/step - loss: 0.0740 - acc: 0.9872 - val_loss: 0.2579 - val_acc: 0.9250\n",
      "Epoch 149/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.0578 - acc: 1.0000 - val_loss: 0.2600 - val_acc: 0.9250\n",
      "Epoch 150/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.0504 - acc: 1.0000 - val_loss: 0.2625 - val_acc: 0.9250\n",
      "Epoch 151/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.0709 - acc: 0.9936 - val_loss: 0.2641 - val_acc: 0.9250\n",
      "Epoch 152/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.0599 - acc: 1.0000 - val_loss: 0.2652 - val_acc: 0.9250\n",
      "Epoch 153/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.0550 - acc: 1.0000 - val_loss: 0.2662 - val_acc: 0.9250\n",
      "Epoch 154/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.0571 - acc: 1.0000 - val_loss: 0.2672 - val_acc: 0.9250\n",
      "Epoch 155/300\n",
      "2/2 [==============================] - 7s 2s/step - loss: 0.0520 - acc: 1.0000 - val_loss: 0.2676 - val_acc: 0.9250\n"
     ]
    }
   ],
   "source": [
    "es_callback = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "history = model.fit(X_train_sc, y_train, validation_split=0.2, batch_size=128, epochs=300, callbacks=[es_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 181ms/step\n",
      "3/3 [==============================] - 1s 179ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((84, 1), (84, 1))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_val = model.predict(X_val_sc)\n",
    "p_test = model.predict(X_test_sc)\n",
    "p_val.shape, p_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 : 0.6785714285714286\n",
      "f1 : 0.12903225806451613\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, ConfusionMatrixDisplay, roc_auc_score, roc_curve\n",
    "\n",
    "p_val_bin = p_val>=0.1\n",
    "\n",
    "print('정확도 :', accuracy_score(y_test, p_val_bin))\n",
    "print('f1 :', f1_score(y_test, p_val_bin))\n",
    "\n",
    "plt.figure()\n",
    "ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_test, p_val_bin), display_labels=['Good', 'Bad']).plot(cmap='Blues')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fpr, tpr, _ = roc_curve(y_test, p_test)\n",
    "auc = roc_auc_score(y_test, p_test)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr)\n",
    "plt.plot([0,1], [0,1], linestyle='--')\n",
    "plt.text(0.5, 0.3, f'AUC = {auc:.4f}', fontsize=20)\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_28\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_31 (InputLayer)       [(None, 7005, 11)]        0         \n",
      "                                                                 \n",
      " conv1d_101 (Conv1D)         (None, 7005, 32)          1088      \n",
      "                                                                 \n",
      " max_pooling1d_5 (MaxPoolin  (None, 3503, 32)          0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " conv1d_102 (Conv1D)         (None, 3503, 16)          1552      \n",
      "                                                                 \n",
      " max_pooling1d_6 (MaxPoolin  (None, 1752, 16)          0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " conv1d_103 (Conv1D)         (None, 1752, 16)          784       \n",
      "                                                                 \n",
      " up_sampling1d_6 (UpSamplin  (None, 3504, 16)          0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " conv1d_104 (Conv1D)         (None, 3504, 32)          1568      \n",
      "                                                                 \n",
      " up_sampling1d_7 (UpSamplin  (None, 7008, 32)          0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " conv1d_105 (Conv1D)         (None, 7008, 11)          1067      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6059 (23.67 KB)\n",
      "Trainable params: 6059 (23.67 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, UpSampling1D\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# 입력 형태\n",
    "input_shape = (7005, 11)\n",
    "\n",
    "# 인코더\n",
    "input_seq = Input(shape=input_shape)\n",
    "x = Conv1D(32, 3, activation='relu', padding='same')(input_seq)\n",
    "x = MaxPooling1D(2, padding='same')(x)\n",
    "x = Conv1D(16, 3, activation='relu', padding='same')(x)\n",
    "encoded = MaxPooling1D(2, padding='same')(x)\n",
    "\n",
    "# 디코더\n",
    "x = Conv1D(16, 3, activation='relu', padding='same')(encoded)\n",
    "x = UpSampling1D(2)(x)\n",
    "x = Conv1D(32, 3, activation='relu', padding='same')(x)\n",
    "x = UpSampling1D(2)(x)\n",
    "decoded = Conv1D(11, 3, activation='sigmoid', padding='same')(x)\n",
    "\n",
    "# 오토인코더 모델\n",
    "autoencoder = Model(input_seq, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='mse', metrics='acc')\n",
    "autoencoder.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\jjong\\Desktop\\vscode\\CWNU_ICT\\tensor\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\jjong\\Desktop\\vscode\\CWNU_ICT\\tensor\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1322, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\jjong\\Desktop\\vscode\\CWNU_ICT\\tensor\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1303, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\jjong\\Desktop\\vscode\\CWNU_ICT\\tensor\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1081, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"c:\\Users\\jjong\\Desktop\\vscode\\CWNU_ICT\\tensor\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1139, in compute_loss\n        return self.compiled_loss(\n    File \"c:\\Users\\jjong\\Desktop\\vscode\\CWNU_ICT\\tensor\\Lib\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"c:\\Users\\jjong\\Desktop\\vscode\\CWNU_ICT\\tensor\\Lib\\site-packages\\keras\\src\\losses.py\", line 142, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"c:\\Users\\jjong\\Desktop\\vscode\\CWNU_ICT\\tensor\\Lib\\site-packages\\keras\\src\\losses.py\", line 268, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"c:\\Users\\jjong\\Desktop\\vscode\\CWNU_ICT\\tensor\\Lib\\site-packages\\keras\\src\\losses.py\", line 1608, in mean_squared_error\n        return backend.mean(tf.math.squared_difference(y_pred, y_true), axis=-1)\n\n    ValueError: Dimensions must be equal, but are 7008 and 7005 for '{{node mean_squared_error/SquaredDifference}} = SquaredDifference[T=DT_FLOAT](model_28/conv1d_105/Sigmoid, IteratorGetNext:1)' with input shapes: [?,7008,11], [?,7005,11].\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[125], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m es_callback \u001b[39m=\u001b[39m EarlyStopping(monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m, patience\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, restore_best_weights\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m----> 2\u001b[0m history \u001b[39m=\u001b[39m autoencoder\u001b[39m.\u001b[39;49mfit(X_train_sc, X_train_sc, validation_split\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m128\u001b[39;49m, epochs\u001b[39m=\u001b[39;49m\u001b[39m300\u001b[39;49m, callbacks\u001b[39m=\u001b[39;49m[es_callback])\n",
      "File \u001b[1;32mc:\\Users\\jjong\\Desktop\\vscode\\CWNU_ICT\\tensor\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file2jqpt6xe.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\jjong\\Desktop\\vscode\\CWNU_ICT\\tensor\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\jjong\\Desktop\\vscode\\CWNU_ICT\\tensor\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1322, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\jjong\\Desktop\\vscode\\CWNU_ICT\\tensor\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1303, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\jjong\\Desktop\\vscode\\CWNU_ICT\\tensor\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1081, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"c:\\Users\\jjong\\Desktop\\vscode\\CWNU_ICT\\tensor\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1139, in compute_loss\n        return self.compiled_loss(\n    File \"c:\\Users\\jjong\\Desktop\\vscode\\CWNU_ICT\\tensor\\Lib\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"c:\\Users\\jjong\\Desktop\\vscode\\CWNU_ICT\\tensor\\Lib\\site-packages\\keras\\src\\losses.py\", line 142, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"c:\\Users\\jjong\\Desktop\\vscode\\CWNU_ICT\\tensor\\Lib\\site-packages\\keras\\src\\losses.py\", line 268, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"c:\\Users\\jjong\\Desktop\\vscode\\CWNU_ICT\\tensor\\Lib\\site-packages\\keras\\src\\losses.py\", line 1608, in mean_squared_error\n        return backend.mean(tf.math.squared_difference(y_pred, y_true), axis=-1)\n\n    ValueError: Dimensions must be equal, but are 7008 and 7005 for '{{node mean_squared_error/SquaredDifference}} = SquaredDifference[T=DT_FLOAT](model_28/conv1d_105/Sigmoid, IteratorGetNext:1)' with input shapes: [?,7008,11], [?,7005,11].\n"
     ]
    }
   ],
   "source": [
    "es_callback = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "history = autoencoder.fit(X_train_sc, X_train_sc, validation_split=0.2, batch_size=128, epochs=300, callbacks=[es_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 2s 429ms/step\n",
      "3/3 [==============================] - 1s 417ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((84, 11), (84, 11))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_val = AE.predict(X_val_sc)\n",
    "p_test = AE.predict(X_test_sc)\n",
    "p_val.shape, p_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (84,7005,11) (84,11) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[98], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreconstruction_error\u001b[39m(x, p):\n\u001b[0;32m      3\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mmean(np\u001b[39m.\u001b[39mabs(x \u001b[39m-\u001b[39m p), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m rce \u001b[39m=\u001b[39m reconstruction_error(X_val_sc, p_val)\n\u001b[0;32m      7\u001b[0m plt\u001b[39m.\u001b[39mfigure()\n\u001b[0;32m      8\u001b[0m sns\u001b[39m.\u001b[39mhistplot(x\u001b[39m=\u001b[39mrce, hue\u001b[39m=\u001b[39my_val, bins\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m, kde\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[98], line 3\u001b[0m, in \u001b[0;36mreconstruction_error\u001b[1;34m(x, p)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreconstruction_error\u001b[39m(x, p):\n\u001b[1;32m----> 3\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mmean(np\u001b[39m.\u001b[39mabs(x \u001b[39m-\u001b[39;49m p), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (84,7005,11) (84,11) "
     ]
    }
   ],
   "source": [
    "# 재건에러\n",
    "def reconstruction_error(x, p):\n",
    "    return np.mean(np.abs(x - p), axis=1)\n",
    "\n",
    "rce = reconstruction_error(X_val_sc, p_val)\n",
    "\n",
    "plt.figure()\n",
    "sns.histplot(x=rce, hue=y_val, bins=50, kde=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84, 7005, 11)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
